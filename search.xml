<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PyTorch：卷积神经网络</title>
      <link href="/2020/03/04/pytorch-juan-ji-shen-jing-wang-luo-jian-jie/"/>
      <url>/2020/03/04/pytorch-juan-ji-shen-jing-wang-luo-jian-jie/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=31473269&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h1><p>卷积神经网络是由一个或多个卷积层和顶端的全连接层（也可以使用1×1的卷积层作为最终的输出）组成一种前馈神经网络。一般的认为，卷积神经网络是由Yann LeCun在1989年提出的LeNet中首先被使用，但是由于当时的计算能力不够，并没有得到广泛的应用，到了1998年Yann LeCun及其合作者构建了更加完备的卷积神经网络LeNet-5并在手写数字识别的问题中取得成功，LeNet-5基本上定义了现代卷积神经昂罗的基本结构，其构筑中交替出现的卷积层-池化层被认为有效提取了图像的平移不变特征，使得对于特征的提取前进了一大步，所以我们一般的认为，Yann LeCun是卷积神经网络的创始人。</p><p>2006年后，随着深度学习理论的完善，尤其是计算能力的提升和参数微调（fine-tuning)等技术的出现，卷积神经网络开始快速发展，在结构上不断加深，各类学习和优化理论得到引入，2012年的AlexNet、2014年的VGGNet、GoogLeNet和2015年的ResNet，使得卷积神经网络几乎成为了深度学习中图像处理方面的标配。</p><h2 id="为什么要使用卷积神经网络"><a href="#为什么要使用卷积神经网络" class="headerlink" title="为什么要使用卷积神经网络"></a>为什么要使用卷积神经网络</h2><p>对于计算机视觉来说，每一个图像是由一个个像素点构成，每个像素点有三个通道，分别代表RGB三种颜色（不计算透明度），我们以手写识别的数据集MNIST举例，每个图像的是一个长度为28，channel为1的单色图像，如果使用全连接的网络结构，即，网络中的神经与相邻层上的每个神经元均连接，那就意味着我们的网络有28×28=784个神经元（RGB3色的话还要3），hidden层如果使用了15个神经元，需要的参数个数（w和b）就有：28×28×15×10+15+10=117625个，这个数量级到现在为止也是一个很恐怖的事，一次反向传播计算量都是巨大的，这还展示一个单色的28像素大小的图片，如果我们使用更大的像素，计算量可想而知。</p><h2 id="结构组成"><a href="#结构组成" class="headerlink" title="结构组成"></a>结构组成</h2><p>上面说到传统的网络需要大量的参数，但是这些参数是否重复了呢，例如，我们识别一个人，只要看到他的眼睛，鼻子，嘴，还有脸基本上就知道这个人是谁了，只使用这些局部特征就能做判断了，并不需要所有的特征。另外一点就是我们上面说的可以有效提取了输入图像的平移不变特征，就好像我们看到了这个是眼睛，这个眼睛在左边还是在右边都是眼睛，这就是平移不变性。我们通过卷积的计算操作来提取图像局部的特征，每一层都会计算出一些局部特征，这些局部特征再汇总到下一层，这样一层层的传递下去，特征有小变大，最后在通过这些局部的特征对图片进行处理，这样大大提高了计算效率，也提高了准确度。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><h4 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h4><p>在介绍卷积层之前要先介绍一种卷积的技术，这里使用<a href="https://www.zhihu.com/question/39022858" target="_blank" rel="noopener">知乎</a>上的一张图片<br><img src="%E5%8D%B7%E7%A7%AF.gif" alt><br>我们会定义一个权重矩阵，也就是我们说的W(一般对于卷积来说，称做卷积的kernel也有有人称做过滤器filter），这个权重矩阵的大小一般为3×3或者5×5，但是在LeNet里面还用到了比较大的7×7，现在已经很少见了，因为根据经验的验证3和5是最佳的大小。我们以图上所示的方式，我们再输入矩阵上使用我们的权重矩阵进行滑动，没滑动一步，将所覆盖的值与矩阵对应的值相乘，并将结构求和并作为输出矩阵的一项，依此类推直到全部计算完成。</p><p>上图所示，我们输入是一个5×5的矩阵，通过使用一次的3<em>3的卷积核计算得到的计算结果是一个3</em>3的新矩阵。那么新矩阵的大小是如何计算的呢？</p><h4 id="卷积核大小f"><a href="#卷积核大小f" class="headerlink" title="卷积核大小f"></a>卷积核大小f</h4><p>刚才已经说到了一个重要的参数，就是核的大小，我们这里用f来表示</p><h4 id="边界填充（p-adding"><a href="#边界填充（p-adding" class="headerlink" title="边界填充（p)adding"></a>边界填充（p)adding</h4><p>我们看到上图，经过计算后矩阵的大小改变了，如果要使矩阵大小不改变呢，我们可以先对矩阵做一个填充，将矩阵的周围全部再包围一层，这个矩阵就变成了7<em>7，上下左右各加一，相当于5+1+1=7，这时，计算的结果还是5</em>5的矩阵，保证了大小不变，这里的p=1</p><h4 id="步长（s-tride"><a href="#步长（s-tride" class="headerlink" title="步长（s)tride"></a>步长（s)tride</h4><p>从动图上我们能够看到，每次滑动只是滑动了一个距离，如果每次滑动两个距离呢？那就需要使用步长这个参数。</p><h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><p>n为我们输入的矩阵的大小，(n-f+2p)/s+1向下取整<br>这个公式非常重要一定要记住</p><h4 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层"></a>卷积层</h4><p>在每一个卷积层中我们都会设置多个核，每个核代表着不同的特征，这些特征就是我们需要传递到下一层的输出，而我们训练的过程就是训练这些不同的核。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>由于卷积的操作也是线性的，所以也需要进行激活，一般情况下，都会使用relu。</p><h3 id="池化层（pooling"><a href="#池化层（pooling" class="headerlink" title="池化层（pooling)"></a>池化层（pooling)</h3><p>池化层是CNN的重要组成部分，通过减少卷积层之间的连接，降低运算复杂程度，池化层的操作很简单，就相当于是合并，我们输入一个过滤器的大小，与卷积的操作一样，也是一步步滑动，但是过滤器覆盖的区域进行合并，只保留一个值。合并的方式也有很多种。例如我们常用的两种取最大值maxpooling，取平均值avgpooling池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以p=0，可以简化为（n-f）/ s + 1</p><h4 id="dropout层"><a href="#dropout层" class="headerlink" title="dropout层"></a>dropout层</h4><p>dropout是2014年Hinton提出防止过拟合而采用的trick，增强了模型的泛化能力Dropout（随机失活）是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始网络中找到一个更瘦的网络，说的通俗一点，就是随机将一部分网络的传播掐断，听起来好像不靠谱，但是通过实际测试效果非常好。有兴趣的可以去看一下原文<a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>.</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>全连接层一般是作为最后的输出层使用，卷积的作用是提取图像的特征，最后的全连接层就是要通过这些特征来进行计算，输出我们所要的结果了，无论是分类，还是回归。<br>我们的特征都是使用矩阵表示的，所以在传入全连接层之前还需要对特征进行压扁，将他这些特征变成一位的向量，如果进行分类的话，就是用sofmax作为输出，如果要使回归的话就直接使用linear即可。<br>以上就是卷积神经网络的几个重要组成部分，下面我们介绍一些经典的网络模型</p><h2 id="经典模型"><a href="#经典模型" class="headerlink" title="经典模型"></a>经典模型</h2><p>LeNet-5<br>1998,Yann LeCun的LeNet5<a href="http://yann.lecun.com/exdb/lenet/index.html" target="_blank" rel="noopener">官网</a><br>卷积神经网络的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件</p><ul><li>用卷积层提取空间特征；</li><li>由空间平均得到子样本；</li><li>用tanh或sigmoid得到非线性</li><li>用multi-layer neural network(MLP)作为最终的分类器；</li><li>层层之间用稀疏的连接矩阵，以避免大的计算成本。<br><img src="lenet5.jpg" alt><br>输入：图像Size为32<em>32.这要比minist数据库中最大的字母（28</em>28）还大。这样做的目的是希望潜在的明显特征，如笔画断续、角点能够出现在最高层特征监测子感受野的中心。<br>输出：10个类别，分别是0-9数字的概率  </li></ul><p>1.C1层是一个卷积层，有6个卷积核（提取6中局部特征），核大小为5×5<br>2.S2层是pooling层，下采样（区域2×2）降低网络训练参数及模型的过拟合程度<br>3.C3层是第二个卷积层，使用16个卷积核，核大小：5×5提取特征<br>4.S4曾也是一个pooling层，区域：2×2<br>5.C5层是最后一个卷积层，卷积核大小：5×5卷积核种类：120<br>6.最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率<br>以下代码来自<a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" target="_blank" rel="noopener">官方教程</a></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">LeNet5</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>LeNet5<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 1 input image channel, 6 output channels, 5×5 square convolution</span>        <span class="token comment" spellcheck="true"># kernel</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># an affine operation: y = Wx + b</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#论文是conv，官方教程用了线性层</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Max pooling over a (2, 2) window</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#If the size is a square you can only specify a single number</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_flat_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">num_flat_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#all dimensions except the batch dimension</span>        num_fearures <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> size<span class="token punctuation">:</span>            num_fearuens <span class="token operator">*=</span> s        <span class="token keyword">return</span> num_featuresnet <span class="token operator">=</span> LeNet5<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>LeNet5(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>2012,<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Alex Krizhevsky</a>可以算作LeNet的一个更深和更广的版本，可以用来学习重复复杂的对象。</p><ul><li>用rectified linear units（ReLU)得到非线性；  </li><li>使用dropout技巧在训练期间有选择性地忽略单个神经元，来减缓模型的过拟合；  </li><li>重叠最大池，避免平均池的平均效果；  </li><li>使用GPU NVIDIA GTX 580可以减少训练时间，这比用CPU处理快了10倍，所以可以被用于更大的数据集和图像上。<br><img src="AlexNet.png" alt>  </li></ul><p>虽然AlexNet只有8层，但是它有60M以上的参数总量，Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理，这里就不作详细介绍了，AlexNet的每一阶段（含一次卷积主要计算的算作一层）可以分为8层：<br>1.con - relu - pooling - LRN ： 要注意的是input层是227×227，而不是paper里面的224，这里可以算一下，主要是227可以整除后面的conv1计算，224不整除。如果一定要用224可以通过自动补边实现，不过在input就补边感觉没有意义，补得也是0，这就是我们上面说的公式的重要性。<br>2.conv-relu-pool-LRN:group=2,这个属性强行把前面结果的feature map分开，卷积部分分成两部分做。<br>3.conv-relu<br>4.conv-relu<br>5.conv-relu-pool<br>6.fc-relu-droupout：droupout层，在alexnet中是说在训练的以1/2概率使得隐藏层的某些neuron的输出为0，这样就丢掉了一半节点的输出，BP的时候也不更新这些节点，防止过拟合。<br>7.fc-relu-dropout<br>8.fc-softmax<br>在PyTorch的vision包中是包含AlexNet的官方实现的，我们直接使用官方版本查看网络架构、</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>alexnet<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=4096, out_features=4096, bias=True)    (5): ReLU(inplace=True)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))</code></pre><p># </p><h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p>2015,牛津的<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">VGG</a></p><ul><li>每个卷积层中使用更小的3×3filters，并将它们合并组成卷积序列</li><li>多个3×3卷积序列可以模拟更大的接收场的效果</li><li>每次的图像像素缩小一倍，卷积核的数量增加一倍</li></ul><p>VGG有多个版本，也算是比较稳定和经典的模型。它的特点是连续conv多计算量巨大，我们这里以VGG16为例。<br><img src="vgg16.png" alt><br>VGG清一色用小卷积核，结合<a href="https://github.com/xxxnhb/pytorch-handbook/blob/master/chapter2/2.4-cnn.ipynb" target="_blank" rel="noopener">pytorch-handbook</a>的观点，这里整理出小卷积核比用大卷积核的优势：</p><p>input8 -&gt; 3层conv3x3后，output=2，等同于1层conv7x7的结果； input=8 -&gt; 2层conv3x3后，output=4，等同于1层conv5x5的结果</p><p>卷积层的参数减少。相比5×5、7×7和11×11的大卷积核，3×3明显地减少了参数量  </p><p>通过卷积和池化层后，图像的分辨率将为原来的一半，但是图像的特征增加了一倍，这是一个十分规整的操作：分辨率由输入的224-&gt;112-&gt;56-&gt;28-&gt;14-&gt;7,特征从原始的RGB3个通道-&gt;64-&gt;128-&gt;256-&gt;512</p><p>PyTorch官方实现版本</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>VGG(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace=True)    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU(inplace=True)    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (6): ReLU(inplace=True)    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (8): ReLU(inplace=True)    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (13): ReLU(inplace=True)    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (15): ReLU(inplace=True)    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (18): ReLU(inplace=True)    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (20): ReLU(inplace=True)    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (22): ReLU(inplace=True)    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (25): ReLU(inplace=True)    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (27): ReLU(inplace=True)    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (29): ReLU(inplace=True)    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))  (classifier): Sequential(    (0): Linear(in_features=25088, out_features=4096, bias=True)    (1): ReLU(inplace=True)    (2): Dropout(p=0.5, inplace=False)    (3): Linear(in_features=4096, out_features=4096, bias=True)    (4): ReLU(inplace=True)    (5): Dropout(p=0.5, inplace=False)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))</code></pre><h1 id="GoogLeNet-Inception"><a href="#GoogLeNet-Inception" class="headerlink" title="GoogLeNet(Inception)"></a>GoogLeNet(Inception)</h1><p>2014,<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Google Christian Szegedy</a></p><ul><li>使用1×1卷积块（NiN)来减少特征数量，这通常被称为“瓶颈”，可以减少神经网络的计算负担。  </li><li>每个池化层之前，增加feature maps，增加每一层的宽度来增多特征的组合性</li></ul><p>googlenet最大的特点就是包含若干个inception模块，所以有时候也称作inceptionnet。googlenet虽然层数上比vgg多很多，但是由于inception的设计，计算速度要快很多<br><img src="googlenet.png" alt><br>Inception架构的主要思想是找到如何让已有的稠密组件接近与覆盖卷积视觉网络中的最佳局部稀疏结构。现在需要找出最优的局部构造，并且重复几次。之前的一篇文献提出一个层与层的结构，在最后一层进行相关性统计，将高相关性的聚集到一起。这些聚类构成下一层的单元，且与上一层单元链接。假设前面层的每个单元对应于输入图像的某些区域，这些单元被分为滤波器组。在接近输入层的低层中，相关单元集中在某些局部区域，最终得到在单个区域中的大量聚类，在最后一层通过1×1的卷积覆盖。</p><p>上面的话听起来很生硬，其实解释起来很简单：每一模块我们都是用若干个不同的特征提取方式，例如3×3卷积，5×5卷积，1×1卷积，pooling等，都计算一下，最后再把这些结果通过Filter Concat来进行连接，找到这里面作用最大的。而网络里面包含了许多这样的模块，这样不用我们人为去判断哪个特征提取方式好，网络会自己解决（有点像AUTO ML),在PyTorch中实现了InceptionA-E,还有InceptionAUX模块。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># inception_v3 需要scipy，所以没有安装的话可以提前安装scipy</span><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>inception_v3<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Inception3(  (Conv2d_1a_3x3): BasicConv2d(    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_2a_3x3): BasicConv2d(    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_2b_3x3): BasicConv2d(    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_3b_1x1): BasicConv2d(    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_4a_3x3): BasicConv2d(    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Mixed_5b): InceptionA(    (branch1x1): BasicConv2d(      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_1): BasicConv2d(      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_2): BasicConv2d(      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_5c): InceptionA(    (branch1x1): BasicConv2d(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_1): BasicConv2d(      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_2): BasicConv2d(      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_5d): InceptionA(    (branch1x1): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_1): BasicConv2d(      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_2): BasicConv2d(      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6a): InceptionB(    (branch3x3): BasicConv2d(      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6b): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6c): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6d): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6e): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (AuxLogits): InceptionAux(    (conv0): BasicConv2d(      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (conv1): BasicConv2d(      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (fc): Linear(in_features=768, out_features=1000, bias=True)  )  (Mixed_7a): InceptionD(    (branch3x3_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2): BasicConv2d(      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_2): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_3): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_4): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_7b): InceptionE(    (branch1x1): BasicConv2d(      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_1): BasicConv2d(      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_7c): InceptionE(    (branch1x1): BasicConv2d(      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_1): BasicConv2d(      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (fc): Linear(in_features=2048, out_features=1000, bias=True))</code></pre><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>2015,<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Kaiming He,Xiangyu Zhang,Shaoqing Ren,Jian Sun.</a> </p><p>Kaiming He这个大神大家一定要记住,现在很多论文都有他参与（mask rcnn，focal loss），Jian Sun老师，旷视科技的首席科学家。googlenet已经很深了，ResNrt可以做到更深，通过残差计算，可以训练超过1000层的网络，俗称跳连接</p><h2 id="退化问题"><a href="#退化问题" class="headerlink" title="退化问题"></a>退化问题</h2><p>网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。这个就是网络退化的问题，退化问题说明了深度网络不能很简单地被很好地优化</p><h2 id="残差网络的解决方法"><a href="#残差网络的解决方法" class="headerlink" title="残差网络的解决方法"></a>残差网络的解决方法</h2><p>深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难。如果把网络设计为H(x) = F(x) + x。我们可以转换为学习一个残差函数F(x) = H(x) - x。 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。</p><p>以上又很不好理解，继续解释下，先看图：<br><img src="resnet.png" alt><br>我们在激活函数前将上一层（或几层）的输出与本层计算的输出相加，将求和的结果输入到激活函数中做为本层的输出，引入残差后的映射对输出的变化更敏感，其实就是看本层相对前几层是否有大的变化，相当于是一个差分放大器的作用。图中的曲线就是残差中的shoutcut，他将前一层的结果直接连接到了本层，也就是俗称的跳连接。<br>我们以经典的resnet18来看一下网络结构<br><img src="resnet18.jpg" alt></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>ResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )    (1): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer2): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer3): Sequential(    (0): BasicBlock(      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer4): Sequential(    (0): BasicBlock(      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=512, out_features=1000, bias=True))</code></pre><h1 id="如何选择网络？"><a href="#如何选择网络？" class="headerlink" title="如何选择网络？"></a>如何选择网络？</h1><p><img src="cnn.png" alt><br>以上表格可以清楚的看到准确率和计算量之间的对比.小型图片分类任务，resnet18基本上已经可以了，如果真对准确度要求比较高，再选其他更好的网络架构。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch:数据加载和预处理</title>
      <link href="/2020/03/03/pytorch-shu-ju-de-jia-zai-he-yu-chu-li/"/>
      <url>/2020/03/03/pytorch-shu-ju-de-jia-zai-he-yu-chu-li/</url>
      
        <content type="html"><![CDATA[<h1 id="数据加载和预处理"><a href="#数据加载和预处理" class="headerlink" title="数据加载和预处理"></a>数据加载和预处理</h1><p>PyTorch通过torch.utils.data对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。并且torchvision已经预先实现了常用图像数据集，包括前面使用的CIDFAR-10,ImageNet、COCO、MNIST、LSUN等数据，可以通过torchvison.datasets方便的调用</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 首先要引入相关的包</span><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 打印一下版本</span>torch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>Dataset是一个抽象类，为了能够方便的读取，需要将使用的数据包装为Dataset类。自定义的Dataset需要继承它并且实现两个成员方法：<br>1.<strong>getitem</strong>()该方法定义用索引（0到len（self））获取一条数据或一个样本<br>2.<strong>len</strong>()该方法返回数据集的总长度<br>下面我们使用kaggel上的一个竞赛<a href="https://www.kaggle.com/c/bluebook-for-bulldozers/data" target="_blank" rel="noopener">bluebook for buildozers</a>自定义一个数据集，为了方便介绍，我们使用里面的数据字典来做说明</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 引用</span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#定义一个数据集</span><span class="token keyword">class</span> <span class="token class-name">BulldozerDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""数据集演示"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_file<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""实现初始化方法，在初始化的时候将数据读载入"""</span>        self<span class="token punctuation">.</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>csv_file<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        返回df长度        '''</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>df<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        根据idx返回一行数据        '''</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>df<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">.</span>SalePrice<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>至此，我们的数据集已经定义完成了，我们可以实例化一个对象访问它</p><pre class="line-numbers language-python"><code class="language-python">ds_demo <span class="token operator">=</span> BulldozerDataset<span class="token punctuation">(</span><span class="token string">'./bluebook-for-bulldozers/median_benchmark.csv'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 实现了__len__方法所以可以直接使用len获取数据总数</span>len<span class="token punctuation">(</span>ds_demo<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>11573</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#用索引可以直接访问对应的数据，对应__getitem__方法</span>ds_demo<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>24000.0</code></pre><p>自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据</p><h1 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h1><p>DataLoader为我们提供了对Dataset的读取操作，常用参数有：batch_size(每个batch的大小）、shuffle(是否进行shuffle操作)、num_workers(加载数据的时候使用几个子进程)。下面做一个简单的操作</p><pre class="line-numbers language-python"><code class="language-python">dl <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>ds_demo<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>DataLoader返回的是一个可迭代对象，我们可以使用迭代器分词获取数据</p><pre class="line-numbers language-python"><code class="language-python">idata <span class="token operator">=</span> iter<span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>next<span class="token punctuation">(</span>idata<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,        24000.])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>data<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#为了节约空间，这里只循环一遍</span>    <span class="token keyword">break</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0 tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,        24000.])</code></pre><p>我们以及可以使用dataset定义数据集，并使用Datalorder载入和遍历数据集，除了这些外，PyTorch还提供能torchvision的计算机视觉扩展包，里面封装了torchvision</p><h1 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h1><p>torchvision是PyTorch中专门用来处理图像的库，Pytorch官网的安装教程中最后的pip install torchvision就是安装这个包</p><h2 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h2><p>torchvision.datasets可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用</p><ul><li>MNIST    </li><li>COCO  </li><li>Captions  </li><li>Detection  </li><li>LSUN  </li><li>ImageFolder  </li><li>Imagenet-12  </li><li>CIFAR  </li><li>STL10  </li><li>SVHN  </li><li>Photo Tour我们可以直接使用，示例如下：</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> datasetstrainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#表示MNIST数据的加载的目录</span>                         train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#表示是否加载数据库的训练集，false的时候加载测试集</span>                         download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#表示是否自动下载MNIST数据集</span>                         transform<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#表示是否需要对数据进行预处理，none为不进行预处理</span>                         <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Files already downloaded and verified</code></pre><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者再进行迁移学习torchvision.models模块的子模块中包含以下模型结构。</p><ul><li>AlexNet</li><li>VGG</li><li>ResNet</li><li>SqueezeNet</li><li>DenseNet</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的</span><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> modelsresnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="torchvision-transfroms"><a href="#torchvision-transfroms" class="headerlink" title="torchvision.transfroms"></a>torchvision.transfroms</h3><p>transforms模块提供了一般的图像转换操作类，用作数据处理和数据增强</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms <span class="token keyword">as</span> transformstransform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#先四周填充0，再把图像随机裁剪成32*32</span>    transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 图像一半的改了旋转，一半的概率不旋转</span>    transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">45</span><span class="token punctuation">,</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#随机旋转</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.4914</span><span class="token punctuation">,</span> <span class="token number">0.4822</span><span class="token punctuation">,</span> <span class="token number">0.4465</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span><span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">#R、G、B每层的归一化用到的均值和方差</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010) 这几个数字的理解可以参考<a href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21" target="_blank" rel="noopener">官方</a>。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch:神经网络包和优化器optm</title>
      <link href="/2020/03/01/pytorch-shen-jing-wang-luo-bao-he-you-hua-qi-optm/"/>
      <url>/2020/03/01/pytorch-shen-jing-wang-luo-bao-he-you-hua-qi-optm/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络包和优化器optm"><a href="#神经网络包和优化器optm" class="headerlink" title="神经网络包和优化器optm"></a>神经网络包和优化器optm</h1><p>torch.nn是专门为神经网络设计的模块化接口。nn构建于Autograd之上，可以来定义和运行神经网络。这里我们主要介绍几个一些常用的类</p><p>约定：torch.nn我们为了方便使用，我们会把它设置为别名nn。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 首先要引入相关的包</span><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 引入torch.nn并指定别名</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment" spellcheck="true"># 打印一下版本</span>torch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><p>除了nn别名以外，我们还引用了nn.functional,这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数（ReLU,pool,DropOut等）,这些函数可以放在构造函数中，也可以不放，但是这里建议不放。<br>一般情况下我们会将nn.functional设置为F,方便调用。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h1><p>PyTorch中已经为我们准备好了现成的网络模型，只要继承nn.Model，并实现它的forward方法，PyTorch会根据autograd，自动实现backward函数，在forward函数中可以使用任何支持tensor的函数，还可以使用if、for循环、print、log等Python语法，写法和标准的Python语法一致。、</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># nn.Model子类的函数必须在构造函数中执行父类的构造函数</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 卷积层‘1’表示输入图片为单通道， ‘6’表示输出通道数， ‘3’表示卷积核为3*3</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 线性层，输入1350个特征，输出10个特征</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1350</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这里的1350根据后面forwar函数计算</span>    <span class="token comment" spellcheck="true">#正向传播</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 结果：【1， 1， 32， 32】</span>        <span class="token comment" spellcheck="true">#卷积 -> 激活 -> 池化</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 根据卷积的尺寸计算公式，计算结果是30</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#结果：【1，6，30，30】</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 我们使用池化层，计算结果是15</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># 结果：[1, 6, 15, 15]      </span>        <span class="token comment" spellcheck="true"># reshape, '-1'表示自适应</span>        <span class="token comment" spellcheck="true">#这里做的就是压扁操作，就是把后面的【1，6， 15， 15】压扁，变为【1，1350】</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#这里就是fc1层的输入1350</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))  (fc1): Linear(in_features=1350, out_features=10, bias=True))</code></pre><p>网络的可学习参数通过net.parameters()返回</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>parameters<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Parameter containing:tensor([[[[-0.2722,  0.2127,  0.1440],          [ 0.1893,  0.2289, -0.1173],          [ 0.1928,  0.0049, -0.0850]]],        [[[ 0.1248, -0.2400, -0.0258],          [ 0.0049,  0.0040, -0.2947],          [-0.2115,  0.1071,  0.2063]]],        [[[ 0.0656, -0.1588,  0.3105],          [-0.2901, -0.1213,  0.1705],          [-0.1190,  0.2064,  0.3216]]],        [[[ 0.1882,  0.3190,  0.3296],          [-0.3016, -0.2781,  0.1870],          [-0.2218, -0.1386, -0.0738]]],        [[[-0.0169,  0.2492,  0.0894],          [-0.2057, -0.2185,  0.0337],          [-0.2511,  0.1548,  0.0939]]],        [[[-0.0301, -0.2842,  0.3223],          [-0.0803,  0.0106, -0.0678],          [ 0.0725,  0.0646,  0.2251]]]], requires_grad=True)Parameter containing:tensor([-0.0749, -0.1380, -0.0128, -0.0752,  0.0611, -0.1311],       requires_grad=True)Parameter containing:tensor([[ 0.0081,  0.0077,  0.0227,  ..., -0.0003, -0.0174, -0.0037],        [-0.0006, -0.0046,  0.0208,  ...,  0.0218,  0.0025, -0.0128],        [-0.0166, -0.0182, -0.0263,  ...,  0.0062,  0.0203, -0.0041],        ...,        [ 0.0228,  0.0088,  0.0267,  ...,  0.0070, -0.0241,  0.0038],        [ 0.0180, -0.0091, -0.0142,  ...,  0.0152,  0.0247, -0.0197],        [ 0.0235, -0.0028, -0.0111,  ...,  0.0014,  0.0076,  0.0157]],       requires_grad=True)Parameter containing:tensor([-0.0089, -0.0139, -0.0196,  0.0216, -0.0125,  0.0169,  0.0131, -0.0029,        -0.0032,  0.0180], requires_grad=True)</code></pre><p>net.named_parameters可同时返回可学习的参数及名称</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span>parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">':'</span><span class="token punctuation">,</span>parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>conv1.weight : torch.Size([6, 1, 3, 3])conv1.bias : torch.Size([6])fc1.weight : torch.Size([10, 1350])fc1.bias : torch.Size([10])</code></pre><p>forward函数的输入输出都是Tensor</p><pre class="line-numbers language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 这里对应前面forward的输入是32</span>out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>out<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])torch.Size([1, 10])</code></pre><pre class="line-numbers language-python"><code class="language-python">input<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])</code></pre><p>在反向传播前，先要将所有参数的梯度清零</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。<br>也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的input，nn中定义为3维，但是我们人工创建时多增加了一个维度，变成了4维，最前面的1即为batch-size</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss来计算均方误差</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># loss是一个scalar，我们可以直接用item获取到他的python类型的数值</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>29.34039306640625</code></pre><h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>在反向传播计算完成所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法（SGD）的更新策略如下：<br>weight = weight-learning_rate*gradient<br>在torch.optim中实现大多数的优化方法，例如RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 这里调用的时候会打印出我们在forward函数中打印x的大小</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#新建一个优化器，SGD只需要调整的参数和学习率</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#先梯度清零（与net.zero_grad()效果一样）</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#更新参数</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])</code></pre><p>这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习：快速入门</title>
      <link href="/2020/02/29/pytorch-shen-du-xue-xi-kuai-su-ru-men/"/>
      <url>/2020/02/29/pytorch-shen-du-xue-xi-kuai-su-ru-men/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1426087898&auto=1&height=66"></iframe></div><h1 id="PyTorch-深度学习-60分钟快速入门-（官方）"><a href="#PyTorch-深度学习-60分钟快速入门-（官方）" class="headerlink" title="PyTorch 深度学习:60分钟快速入门 （官方）"></a>PyTorch 深度学习:60分钟快速入门 （官方）</h1><p>本章为官方网站的<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">Deep Learning with PyTorch: A 60 Minute Blitz</a>的中文翻译</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><a href="1_tensor_tutorial.ipynb">张量</a></li><li><a href="2_autograd_tutorial.ipynb">Autograd: 自动求导</a></li><li><a href="3_neural_networks_tutorial.ipynb">神经网络</a></li><li><a href="4_cifar10_tutorial.ipynb">训练一个分类器</a></li><li><a href="https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/5_data_parallel_tutorial.ipynb" target="_blank" rel="noopener">选读：数据并行处理</a></li></ol><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>本章中的所有图片均来自于PyTorch官网，版权归PyTorch所有.<br>为方便读者理解，相关内容以传送门形式在上方列出。</p><p><em>*</em> 本篇文章来自<a href="https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md" target="_blank" rel="noopener">Pytorch-handbook</a></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于CIFAR10训练一个分类器</title>
      <link href="/2020/02/28/4-cifar10-tutorial/"/>
      <url>/2020/02/28/4-cifar10-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27646687&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>Using matplotlib backend: agg</code></pre><h1 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h1><p>上一讲中已经看到如何去定义一个神经网络，计算损失值和更新网络的权重。你现在可能在想下一步。</p><h2 id="关于数据？"><a href="#关于数据？" class="headerlink" title="关于数据？"></a>关于数据？</h2><p>一般情况下处理图像、文本、音频数据时，可以使用标准的Python包来加载数据到一个numpy数组中。然后把这个数组转换成torcvh.*Tensor。</p><ul><li>图像可以使用Pilolow，OpenCV</li><li>音频可以使用scipy，librosa</li><li>文本可以使用原始Python和Cython来加载，或者使用NLTK或SpaCyc处理</li></ul><p>特别的，对于图像任务，我们创建了一个包torchvision,它包含了处理一些基本图像数据集的方法。这些数据集包括Imagenet、CIFAR10、MNIST等。除了数据加载以外，torchvision还包含了图像转换器，torchvision.datasets和torch.utils.data.DataLoader。<br>torchvision包不仅提供了巨大的便利，也避免了代码的重复。<br>在这个教程中，我们使用CIFAR10数据集，它有如下10个类别 ：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。CIFAR-10的图像都是 3x32x32大小的，即，3颜色通道，32x32像素。</p><h2 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h2><p>依次按照下列顺序进行：<br>1.使用torchvision加载和归一化CIFAR10训练集和测试机<br>2.定义一个卷积神经网络<br>3.定义损失函数<br>4.在训练集上训练网络<br>5.在测试集上测试网络  </p><h2 id="1-读取和归一化-CIFAR10"><a href="#1-读取和归一化-CIFAR10" class="headerlink" title="1.读取和归一化 CIFAR10"></a>1.读取和归一化 CIFAR10</h2><p>使用torchvision可以非常容易地加载CIFAR10。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>torchvision的输出是[0,1]的PILImage图像，我们把它转换为归一化范围为[-1，1]的张量</p><pre class="line-numbers language-python"><code class="language-python">transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>    <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>trainset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span>train <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                                       download <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> transform <span class="token operator">=</span> transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                                          shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>testset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                                       download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                                         shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span>           <span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>Tips：这里在CIFAR10官网已经提前下载好了数据集，和该文件放在了同一文件下，如电脑中没有CIFAR10数据集，则需将download参数置为True，root不指定参数<br>我们展示一些训练图像。<br><img src="cifar10.png" alt></li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true">#展示图像的函数</span><span class="token keyword">def</span> <span class="token function">imshow</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">:</span>    img <span class="token operator">=</span>img <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token comment" spellcheck="true"># unnormalize</span>    npimg <span class="token operator">=</span> img<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>npimg<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取随机数据</span>dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 展示图象</span>imshow<span class="token punctuation">(</span>torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示图像标签</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>  cat  ship  frog  bird</code></pre><h2 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2.定义一个卷积神经网络"></a>2.定义一个卷积神经网络</h2><p>从之前的神经网络一节复制神经网络代码，并修改为输入3通道图像。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3.定义损失函数和优化器"></a>3.定义损失函数和优化器</h2><p>我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimcriterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4.训练网络"></a>4.训练网络</h2><p>有趣的时刻开始了。我们只需在数据迭代器上循环，将数据输入给网络，并优化。</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>      <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 多批次循环</span>    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 获取输入</span>        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        <span class="token comment" spellcheck="true"># 梯度置0</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 正向传播，反向传播，优化</span>        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#打印状态信息</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2000</span> <span class="token operator">==</span> <span class="token number">1999</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 每2000批次打印一次</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span> <span class="token operator">%</span>                 <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> running_loss <span class="token operator">/</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Finished Training"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[1,  2000] loss: 2.058[1,  4000] loss: 1.772[1,  6000] loss: 1.639[1,  8000] loss: 1.547[1, 10000] loss: 1.485[1, 12000] loss: 1.442[2,  2000] loss: 1.371[2,  4000] loss: 1.357[2,  6000] loss: 1.310[2,  8000] loss: 1.315[2, 10000] loss: 1.281[2, 12000] loss: 1.260Finished Training</code></pre><h2 id="5-在测试集上测试网络"><a href="#5-在测试集上测试网络" class="headerlink" title="5.在测试集上测试网络"></a>5.在测试集上测试网络</h2><p>我们在整个训练集上进行了两次训练，但是我们需要检查网络是否从数据集中学习到有用的东西。通过预测神经网络输出的类标签与实际情况标签进行对比来进行检测。如果预测正确，我们把该样本添加到正确预测列表。第一步，显示测试集中图片并熟悉图片内容。</p><pre class="line-numbers language-python"><code class="language-python">dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>testloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示图片</span>imshow<span class="token punctuation">(</span>torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'GroundTruth: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>让我们看看神经网络认为以上图片是什么。</p><pre class="line-numbers language-python"><code class="language-python">output <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出是10个标签的能量。 一个类别的能量越大，神经网络越认为它是这个类别。所以让我们得到最高能量的标签。</p><pre class="line-numbers language-python"><code class="language-python">_<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Predicted: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>predicted<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span>                              <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Predicted:  horse  ship   dog   cat</code></pre><p>结果看来不错。</p><p>接下来让看看网络在整个测试集上的结果如何。</p><pre class="line-numbers language-python"><code class="language-python">correct <span class="token operator">=</span> <span class="token number">0</span>total <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy of the network on the 10000 test images: %d %%'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>    <span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Accuracy of the network on the 10000 test images: 55 %</code></pre><p>结果看起来不错，至少比随机选择要好，随机选择的正确率为10%。 似乎网络学习到了一些东西。</p><p>在识别哪一个类的时候好，哪一个不好呢？</p><pre class="line-numbers language-python"><code class="language-python">class_correct <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>class_total <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        c <span class="token operator">=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            label <span class="token operator">=</span> labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            class_correct<span class="token punctuation">[</span>label<span class="token punctuation">]</span> <span class="token operator">+=</span> c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            class_total<span class="token punctuation">[</span>label<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy of %5s : %2d %%'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>        classes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">100</span> <span class="token operator">*</span> class_correct<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/</span> class_total<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Accuracy of plane : 63 %Accuracy of   car : 74 %Accuracy of  bird : 17 %Accuracy of   cat : 26 %Accuracy of  deer : 50 %Accuracy of   dog : 49 %Accuracy of  frog : 70 %Accuracy of horse : 75 %Accuracy of  ship : 68 %Accuracy of truck : 55 %</code></pre><p>下一步？<br>我们如何在GPU上运行神经网络呢？</p><h2 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h2><p>把一个神经网络移动到GPU上训练就像把一个Tensor转换GPU上一样简单。并且这个操作会递归遍历有所模块，并将其参数和缓冲区转换为CUDA张量。</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 确认我们的电脑支持CUDA,然后显示CUDA信息</span><span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>cuda:0</code></pre><p>本节的其余部分假定device是CUDA设备。</p><p>然后这些方法将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量：</p><p>net.to(device)<br>记住：inputs, targets 和 images 也要转换。</p><p>inputs, labels = inputs.to(device), labels.to(device)<br>为什么我们没注意到GPU的速度提升很多？那是因为网络非常的小。</p><p>实践: 尝试增加你的网络的宽度（第一个nn.Conv2d的第2个参数，第二个nn.Conv2d的第一个参数，它们需要是相同的数字），看看你得到了什么样的加速。</p><h3 id="实现的目标："><a href="#实现的目标：" class="headerlink" title="实现的目标："></a>实现的目标：</h3><ul><li>深入了解了Pytorch的张量库和神经网络</li><li>训练了一个小网络来分类图片</li></ul><h2 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h2><p>如果你想使用所有的GPU得到更大的加速， 请查看数据并行处理。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络</title>
      <link href="/2020/02/28/3-neural-networks-tutorial/"/>
      <url>/2020/02/28/3-neural-networks-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1422274143&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>使用torch.nn包来构建神经网络<br>上一讲提到了autograd，nn包依赖autograd包来定义模型并求导。<br>一个nn.Model包含了各个层和一个forward(input)方法，该方法返回output。<br>例如：<br>它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。<br>神经网络的典型训练过程如下：<br>1.定义包含一些可学习的参数（或者叫权重）神经网络;<br>2.在数据集上迭代；<br>3.通过神经网络处理输入；<br>4.计算损失（输出结果和正确值的差值）；<br>5.将梯度反向传播回网络的参数；<br>6.更新网络的参数，主要使用如下简单的更新规则：<br>  weight = weight - learning_rate * gradient</p><h1 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h1><p>开始定义一个网络：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#I input image channel, 6 output channels, 5 × 5 squar e convolution</span>        <span class="token comment" spellcheck="true">#kernel</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># an affine operation: y = Wx + b</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#Max pooling over a (2, 2)window</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#If the size is a square you can only specify a single number</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_flat_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">num_flat_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#all dimensions except the batch dimension</span>        num_features <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> size<span class="token punctuation">:</span>            num_features <span class="token operator">*=</span> s        <span class="token keyword">return</span> num_featuresnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>在模型中必须要定义forward函数，backward函数（用来计算梯度）会被autograd自动创建。可以在forward函数中使用任何针对Tensor的操作</p><p>net.parameters()返回可被学习的参数（权重）列表和值</p><pre class="line-numbers language-python"><code class="language-python">params <span class="token operator">=</span> list<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>10torch.Size([6, 1, 5, 5])</code></pre><p>测试随机输入32*32。注：这个网络（LeNet)期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32.</p><pre class="line-numbers language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 0.0270, -0.1490, -0.0597, -0.0841, -0.0060, -0.1835,  0.0107,  0.0680,         -0.0480, -0.0272]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>将所有参数的梯度缓存清零，然后进行随机梯度的反向传播：</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>Note<br><code>torch.nn</code> 只支持小批量输入。整个 <code>torch.nn</code> 包都只支持小批量样本，而不支持单个样本。 例如，<code>nn.Conv2d</code> 接受一个4维的张量， <code>每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）</code>。 如果你有单个样本，只需使用 <code>input.unsqueeze(0)</code> 来添加其它的维数<br>在继续之前，我们回顾一下到目前为止用到的类。<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2></li><li>torch.Tensor:一个用过自动调用backward()实现支持自动梯度计算的多维数组，并且保存关于这个向量的梯度w.r.t.</li><li>nn.Module:神经网络模块。封装参数，移动到GPU上运行、导出、加载等。</li><li>nn.Parameter:一种变量，当把它赋给一个Module时，被自动地注册为一个参数</li><li>autograd.Function:实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个Tensor的操作都回创一个接到创建Tensor和编码其历史的函数的Function节点。<h2 id="重点如下："><a href="#重点如下：" class="headerlink" title="重点如下："></a>重点如下：</h2></li><li>定义一个网络</li><li>处理输入，调用backword<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1>一个损失函数接受一对（output，target）作为输入，计算一个值来估计网络的输出和目标值相差多少。<br>nn包中有很多不同的损失函数。nn.MSELoss是一个比较简单的损失函数，它计算输出和目标间的均方误差，例如：</li></ul><pre class="line-numbers language-python"><code class="language-python">output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> target <span class="token operator">=</span> target<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#使target和output的shape相同</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor(1.1544, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>现在，如果在反向过程中跟随loss，使用它的.grad_fn属性，将看到如下所示的计算图。<br>::<br>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss<br>所以，当我们调用loss.backward()时，整张计算图都会根据loss进行微分，而且图中所有设置为requires_grad=True的张量将会拥有一个随着梯度累积的.grad张量。</p><p>为了说明，让我们后退几步：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>&lt;MseLossBackward object at 0x7f39c7490710&gt;&lt;AddmmBackward object at 0x7f39c7490910&gt;&lt;AccumulateGrad object at 0x7f3a13de12d0&gt;</code></pre><h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>调用loss.backward()获得反向传播的误差。</p><p>但是在调用前需要清楚已存在的梯度，否则梯度将被累加到已存在的梯度。</p><p>现在，我们将调用loss.backward(),并查看conv1层的偏差（bias）项在反向传播前后的梯度</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"conv1.bias.grad before backward"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"conv1.bias.grad after backward"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>conv1.bias.grad before backwardtensor([0., 0., 0., 0., 0., 0.])conv1.bias.grad after backwardtensor([0.0200, 0.0075, 0.0097, 0.0054, 0.0115, 0.0240])</code></pre><p>如何使用损失函数</p><h3 id="稍后阅读："><a href="#稍后阅读：" class="headerlink" title="稍后阅读："></a>稍后阅读：</h3><p>nn包，包含了各种用来构成深度神经网络构建的模块和损失函数，完整的文档请查看<a href="https://pytorch.org/docs/nn" title="Torch.nn" target="_blank" rel="noopener">这里</a></p><h1 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h1><p>在实践中最简单的权重更新规则是随机梯度下降（SGD):<br>weight = weight - learning_rate * gradient  </p><p>我们可以使用简单的Python代码实现这个规则：</p><pre><code>learning_rate = 0.01for f in net.parameters():    f.data.sub_(f.grad.data * learning_rate)</code></pre><p>但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包torch.optim实现了所有这些规则。使用它们非常简单：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token comment" spellcheck="true">#create your optimizer</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#in your training loop:</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#zero the gradient buffers</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true">#Does the update</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>观察如何使用“optimizer.zero_grad(“手动将梯度缓冲区设置为零。<br>这是因为梯度是按Backprop部分中的说明累积的。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自动求导(Autograd)</title>
      <link href="/2020/02/28/2-autograd-tutorial/"/>
      <url>/2020/02/28/2-autograd-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1398305777&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="Autograd-自动求导机制"><a href="#Autograd-自动求导机制" class="headerlink" title="Autograd:自动求导机制"></a>Autograd:自动求导机制</h1><p>PyTorch中所有神经网络的核心是autograd包。我们先简单介绍一下这个包，然后训练第一个简单的神经网络。<br>autograd包为张量上的所有操作提供了自动求导。它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。<br>示例  </p><h1 id="张量（Tensor"><a href="#张量（Tensor" class="headerlink" title="张量（Tensor)"></a>张量（Tensor)</h1><p>torch.Tensor是这个包的核心类。如果设置。requires_grad为true，那么将会追踪所有对于该张量的操作。当完成计算后通过调用.backward(),自动计算所有的梯度，这个张量的所有梯度将会自动积累到.grad属性。</p><p>要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。  </p><p>为了防止跟踪历史记录（和使用内存）可以将代码块包装在withtouch.no_grad():中。在评估模型时特别有用，因为模型可能具有requires_grad = True的可训练参数，但是我们不需要梯度计算。</p><p>在自动梯度计算中还有另一个重要的类Function.Tensor和Function互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个.grad_fn属性，这个属性引用了一个创建了Tensor的Function(除非这个张量是用户手动创建的，即，这个张量的grad_fn是None).</p><p>如果需要计算导数，你可以在Tensor上调用.bvackward()。如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward（）指定任何参数，<br>但是如果它有更多的元素，你需要指定一个gradient参数来匹配张量的形状。<br><a href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated" target="_blank" rel="noopener">官方文档</a></p><p>具体的后面会有详细说明</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>创建一个张量并设置requires_grad=True用来追踪它的计算历史</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[1., 1.],        [1., 1.]], requires_grad=True)</code></pre><p>对张量进行操作：</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p>结果y已经被计算出来了，所以，grad_fn已经被自动生成了</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&lt;AddBackward0 object at 0x7fbb2e4392d0&gt;</code></pre><p>对y进行一个操作</p><pre class="line-numbers language-python"><code class="language-python">z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> out<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[27., 27.],        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p>.requires_grad_(…)可以改变现有张量的requires_grad属性。如果没有指定的话，默认输入的flag是False。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>a <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>a<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>b <span class="token operator">=</span> <span class="token punctuation">(</span>a <span class="token operator">*</span> a<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>FalseTrue&lt;SumBackward0 object at 0x7fbb2e439ed0&gt;</code></pre><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>反向传播 因为out是一个纯量（scalar），out.backward()等于out.backward(torch.tensor(1)).</p><pre class="line-numbers language-python"><code class="language-python">out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>print gradients d(out)/dx</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[4.5000, 4.5000],        [4.5000, 4.5000]])</code></pre><p>得到矩阵4.5.将out叫做Tensor“o”<br><img src="Autograd.jpg" alt><br>现在让我们来看一下vector-Jacobian product的例子</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token keyword">while</span> y<span class="token punctuation">.</span>data<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>    y <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([-1578.2891,  -373.9413,   691.9827], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>在这个情形中，y不再是个标量。torch.autograd无法直接计算出完整的雅可比行列式，但是如果我们只想要vector-Jacobian product,只需将向量作为参数传入backward：</p><pre class="line-numbers language-python"><code class="language-python">gradients <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.0001</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>gradients<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</code></pre><p>如果.requires_grad=True但是你又不希望进行autograd的计算，那么可以将变量包裹在with torch.no_grad()中：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>TrueTrueFalse</code></pre><ul><li>稍后阅读：<br>autograd 和 Function的<a href="https://pytorch.org/docs/autograd" title="PyTorch" target="_blank" rel="noopener">官方文档</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>张量(Tensor)简介</title>
      <link href="/2020/02/28/1-tensor-tutorial/"/>
      <url>/2020/02/28/1-tensor-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=475530855&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="PyTorch是什么？"><a href="#PyTorch是什么？" class="headerlink" title="PyTorch是什么？"></a>PyTorch是什么？</h1><p>基于Python的科学计算包，服务于以下两种场景</p><ul><li>作为NumPy的替代品，可以使用GPU的强大计算能力</li><li>提供最大的灵活性和高速的深度学习研究平台</li></ul><h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><p>Tensors(张量)</p><p>Tensors与Numpy中的ndarrays类似，但是在PyTorch中Tensors可以使用GPU进行计算。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> print_function<span class="token keyword">import</span> torch<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>创建一个5×3矩阵，但是未初始化</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[-7.2630e-28,  4.5729e-41, -7.2630e-28],        [ 4.5729e-41,         nan,  3.0970e-41],        [ 1.7753e+28,  4.4339e+27,  5.6719e-11],        [ 7.3471e+28,  2.6383e+23,  2.7376e+20],        [ 1.8040e+28,  1.8750e-19,  7.3909e+22]])</code></pre><p>创建一个随机初始化的矩阵</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0.6957, 0.8565, 0.7844],        [0.3198, 0.9460, 0.4244],        [0.7566, 0.7370, 0.6519],        [0.7324, 0.3396, 0.7447],        [0.1074, 0.1667, 0.0326]])</code></pre><p>创建一个0填充的矩阵，数据类型为long</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]])</code></pre><p>创建tensor并使用现有数据初始化</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([5.5000, 3.0000])</code></pre><p>使用现有的张量创建张量。这些方法将重用输入张量的属性，例如，dtype,除非设置新的值进行覆盖</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> x<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#new_*方法来创建对象</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#覆盖dtype!</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>                                    <span class="token comment" spellcheck="true">#d对象的size相同，值和类型发生变化  </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]], dtype=torch.float64)tensor([[ 0.2724, -2.0907, -0.6096],        [-0.3521,  1.9110, -0.0053],        [ 1.6634,  1.4252, -0.0778],        [ 1.1623, -0.3941, -0.8029],        [ 0.8197,  0.0218, -1.7554]])</code></pre><p>获取size</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([5, 3])</code></pre><p>“torch.Size”返回值是tuple类型，支持tuple类型的所有操作</p><p>加法1：</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token operator">+</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>加法2</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>提供输出tensor作为参数(结果被填充到result作为参数）</p><pre class="line-numbers language-python"><code class="language-python">result <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>out<span class="token operator">=</span>result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]])tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>替换（任何以“_”结尾的操作都会用结果替换原变量）</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#add x to y</span>y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>你可以使用与NumPy索引方式相同的操作来进行对张量的操作</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([-2.0907,  1.9110,  1.4252, -0.3941,  0.0218])</code></pre><p>torch.view():可以改变张量的维度和大小</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span>z <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#size -1 从其他维度推断</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>如果你只有一个元素的张量，使用.item()来得到Python数据类型的数值</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([1.0249])1.0248878002166748</code></pre><p>如果你想要了解更多的关于Tensor的操作，可以访问<a href="https://pytorch.org/docs/torch" title="Tensors" target="_blank" rel="noopener">PyTorch官方文档</a></p><h1 id="NumPy转换"><a href="#NumPy转换" class="headerlink" title="NumPy转换"></a>NumPy转换</h1><p>将一个Torch Tensor转换为NumPy数组是一件轻松的事，反之亦然。<br>Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。</p><p>将一个Torch Tensor转换为NumPy数组</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([1., 1., 1., 1., 1.])</code></pre><pre class="line-numbers language-python"><code class="language-python">b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>[1. 1. 1. 1. 1.]</code></pre><p>观察numpy数组的值是如何改变的。</p><pre class="line-numbers language-python"><code class="language-python">a<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([2., 2., 2., 2., 2.])[2. 2. 2. 2. 2.]</code></pre><p>NumPy Array转化成Torch Tensor  </p><p>使用from_numpy自动转化</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span>np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> out<span class="token operator">=</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[2. 2. 2. 2. 2.]tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><p>所有的Tensor类型默认都是基于CPU，CharTensor类型不支持到NumPy的转换。</p><h1 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h1><p>使用.to方法可以将Tensor移动到任何设备中</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#is_available函数判断是否有cuda可以使用</span><span class="token comment" spellcheck="true">#``torch.device``将张量移动到指定的设备中</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true">#a CUDA 设备对象</span>    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#直接从GPU创建张量</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                         <span class="token comment" spellcheck="true">#直接使用``.to（"cuda"）``将张量移动到cuda中</span>    z <span class="token operator">=</span> x <span class="token operator">+</span> y    <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true">#``.to``也会对变量的类型做更改</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([2.0249], device=&#39;cuda:0&#39;)tensor([2.0249], dtype=torch.float64)</code></pre><p>感谢您的浏览</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch简介及环境搭建</title>
      <link href="/2020/02/27/pytorch-jian-jie-ji-huan-jing-da-jian/"/>
      <url>/2020/02/27/pytorch-jian-jie-ji-huan-jing-da-jian/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1417442423&auto=1&height=66"></iframe></div><p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。</p><h1 id="1-Pytorch简介"><a href="#1-Pytorch简介" class="headerlink" title="1.Pytorch简介"></a>1.Pytorch简介</h1><hr><p>很多人都会拿PyTorch和Google的Tensorflow进行比较，它们是最火的两个深度学习框架了。<br>谈到PyTorch，我们应该先说<a href="http://torch.ch" target="_blank" rel="noopener">Torch</a>。Torch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。<br>PyTorch和Torch使用包含所有相同性能的C库：TH, THC, THNN, THCUNN，并且它们将继续共享这些库。<br>这样的回答就很明确了，其实PyTorch和Torch都使用的是相同的底层，只是使用了不同的上层包装语言。</p><p>PyTorch是一个Python包，提供两个高级功能：</p><ul><li>具有强大的GPU加速的张量计算（如NumPy）</li><li>包含自动求导系统的的深度神经网络</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>PyTorch是相当简洁优雅且高效快速的框架</li><li>设计追求最少的封装，尽量避免重复造轮子</li><li>算是所有的框架中面向对象设计的最优雅的一个，设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法</li><li>大佬支持,与google的Tensorflow类似，FAIR的支持足以确保PyTorch获得持续的开发更新</li><li>不错的的文档（相比FB的其他项目，PyTorch的文档简直算是完善了，参考Thrift），PyTorch作者亲自维护的论坛 供用户交流和求教问题</li><li>入门简单</li></ul><h1 id="2-Pytorch环境搭建"><a href="#2-Pytorch环境搭建" class="headerlink" title="2.Pytorch环境搭建"></a>2.Pytorch环境搭建</h1><p>PyTorch的安装十分简单，根据<a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch官网</a>，对系统选择和安装方式等灵活选择即可。<br>这里以<a href="https://www.anaconda.com/" target="_blank" rel="noopener">anaconda</a>为例，简单的说一下步骤和要点。<br>国内安装anaconda建议使用<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="noopener">清华</a>镜像。</p><h2 id="2-1安装Pytorch"><a href="#2-1安装Pytorch" class="headerlink" title="2.1安装Pytorch"></a>2.1安装Pytorch</h2><p>pytorch的安装经过了几次变化，请大家以官网的安装命令为准。另外需要说明的就是在1.2版本以后，pytorch只支持cuda 9.2以上了，所以需要对cuda进行升级，目前测试大部分显卡都可以用，包括笔记本的MX250也是可以顺利升级到cuda 10.1。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#默认 使用 cuda10.1</span>pip <span class="token function">install</span> torch<span class="token operator">==</span><span class="token operator">=</span>1.4.0 torchvision<span class="token operator">==</span><span class="token operator">=</span>0.5.0 -f https://download.pytorch.org/whl/torch_stable.html<span class="token comment" spellcheck="true">#cuda 9.2</span>pip <span class="token function">install</span> torch<span class="token operator">==</span>1.4.0+cu92 torchvision<span class="token operator">==</span>0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html<span class="token comment" spellcheck="true">#cpu版本</span>pip <span class="token function">install</span> torch<span class="token operator">==</span>1.4.0+cpu torchvision<span class="token operator">==</span>0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>验证输入python 进入</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span class="token comment" spellcheck="true"># 得到结果'1.4.0'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-2配置Jupyter-Notebook"><a href="#2-2配置Jupyter-Notebook" class="headerlink" title="2.2配置Jupyter Notebook"></a>2.2配置Jupyter Notebook</h2><p>新建的环境是没有安装安装ipykernel的所以无法注册到Jupyter Notebook中，所以先要准备下环境</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#安装ipykernel</span>conda <span class="token function">install</span> ipykernel<span class="token comment" spellcheck="true">#写入环境</span>python -m ipykernel <span class="token function">install</span>  --name pytorch --display-name <span class="token string">"Pytorch for Deeplearning"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>下一步就是定制 Jupyter Notebook</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#切换回基础环境</span>activate base<span class="token comment" spellcheck="true">#创建jupyter notebook配置文件</span>jupyter notebook --generate-config<span class="token comment" spellcheck="true">## 这里会显示创建jupyter_notebook_config.py的具体位置</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>打开文件，修改</p><pre><code>c.NotebookApp.notebook_dir = &#39;&#39; 默认目录位置c.NotebookApp.iopub_data_rate_limit = 100000000 这个改大一些否则有可能报错</code></pre><h2 id="2-3-测试"><a href="#2-3-测试" class="headerlink" title="2.3 测试"></a>2.3 测试</h2><p>至此 Pytorch 的开发环境安装完成，可以在开始菜单中打开Jupyter Notebook 在New 菜单中创建文件时选择<code>Pytorch for Deeplearning</code> 创建PyTorch的相关开发环境了</p><h2 id="2-4-问题解决"><a href="#2-4-问题解决" class="headerlink" title="2.4 问题解决"></a>2.4 问题解决</h2><h3 id="问题1：启动python提示编码错误"><a href="#问题1：启动python提示编码错误" class="headerlink" title="问题1：启动python提示编码错误"></a>问题1：启动python提示编码错误</h3><p>删除 .python_history <a href="http://tantai.org/posts/install-keras-pytorch-jupyter-notebook-Anaconda-window-10-cpu/" target="_blank" rel="noopener">来源</a></p><h3 id="问题2-默认目录设置不起效"><a href="#问题2-默认目录设置不起效" class="headerlink" title="问题2 默认目录设置不起效"></a>问题2 默认目录设置不起效</h3><p>打开快捷方式，看看快捷方式是否跟这个截图一样，如果是则删除 <code>%USERPROFILE%</code> 改参数会覆盖掉notebook_dir设置，导致配置不起效<br><img src="pic1.png" alt="Alt text"></p><p>本文参考PyTorch中文手册<br>如果你还发现其他问题，请直接留言</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
