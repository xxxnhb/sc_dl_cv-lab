<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PyTorch：logistic回归实战</title>
      <link href="/2020/03/19/pytorch-logistic-hui-gui-shi-zhan/"/>
      <url>/2020/03/19/pytorch-logistic-hui-gui-shi-zhan/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> numpy <span class="token keyword">as</span> nptorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="logistic回归实战"><a href="#logistic回归实战" class="headerlink" title="logistic回归实战"></a>logistic回归实战</h1><p>本文将处理结构化数据，并使用logistic回归对结构化数据进行简单的分类</p><h2 id="logistic回归介绍"><a href="#logistic回归介绍" class="headerlink" title="logistic回归介绍"></a>logistic回归介绍</h2><p>logistic回归是一种广义线性回归（generalized linear model），与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 wx + b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将wx+b作为因变量，即y =wx+b,而logistic回归则通过函数L将wx+b对应一个隐状态p，p =L(wx+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。</p><p>说的更通俗一点，就是logistic回归会在线性回归后再加一层logistic函数的调用。</p><p>logistic回归主要是进行二分类预测，我们在激活函数时候讲到过 Sigmod函数，Sigmod函数是最常见的logistic函数，因为Sigmod函数的输出的是是对于0~1之间的概率值，当概率大于0.5预测为1，小于0.5预测为0。</p><p>下面我们就来使用公开的数据来进行介绍</p><h2 id="UCI-German-Credit-数据集"><a href="#UCI-German-Credit-数据集" class="headerlink" title="UCI German Credit 数据集"></a>UCI German Credit 数据集</h2><p>UCI German Credit是UCI的德国信用数据集，里面有原数据和数值化后的数据。</p><p>German Credit数据是根据个人的银行贷款信息和申请客户贷款逾期发生情况来预测贷款违约倾向的数据集，数据集包含24个维度的，1000条数据，在这里我们直接使用处理好的数值化的数据，作为展示。<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" target="_blank" rel="noopener">地址</a></p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>我们这里使用的 german.data-numeric是numpy处理好数值化数据，我们直接使用numpy的load方法读取即可</p><pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">=</span> np<span class="token punctuation">.</span>loadtxt<span class="token punctuation">(</span><span class="token string">"german.data-numeric"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>数据读取完成后我们要对数据做一下归一化的处理</p><pre class="line-numbers language-python"><code class="language-python">n<span class="token punctuation">,</span>l <span class="token operator">=</span> data<span class="token punctuation">.</span>shape<span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    meanVal <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>    stdVal <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>    data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>j<span class="token punctuation">]</span><span class="token operator">-</span>meanVal<span class="token punctuation">)</span><span class="token operator">/</span>stdVal<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>打乱数据</p><pre class="line-numbers language-python"><code class="language-python">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>区分训练集和测试集，由于这里没有验证集，所以我们直接使用测试集的准确度作为评判好坏的标准</p><p>区分规则：900条用于训练，100条作为测试</p><p>german.data-numeric的格式为，前24列为24个维度，最后一个为要打的标签（0，1），所以我们将数据和标签一起区分出来</p><pre class="line-numbers language-python"><code class="language-python">train_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">900</span><span class="token punctuation">,</span><span class="token punctuation">:</span>l<span class="token number">-1</span><span class="token punctuation">]</span>train_lab <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">900</span><span class="token punctuation">,</span>l<span class="token number">-1</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span>test_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">900</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span>l<span class="token number">-1</span><span class="token punctuation">]</span>test_lab <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">900</span><span class="token punctuation">:</span><span class="token punctuation">,</span>l<span class="token number">-1</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>下面我们定义模型，模型很简单</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LR</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>LR<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#由于24个维度已经固定了，所以这里写24</span>    <span class="token keyword">def</span> forward（self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span>lab<span class="token punctuation">)</span><span class="token punctuation">:</span>    t <span class="token operator">=</span> pred<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">==</span>lab    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>t<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">net<span class="token operator">=</span>LR<span class="token punctuation">(</span><span class="token punctuation">)</span> criterion<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 使用CrossEntropyLoss损失</span>optm<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Adam优化</span>epochs<span class="token operator">=</span><span class="token number">1000</span> <span class="token comment" spellcheck="true"># 训练1000次</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>下面开始训练了</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 指定模型为训练模式，计算梯度</span>    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 输入值都需要转化成torch的Tensor</span>    x<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>    y<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_lab<span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span>    y_hat<span class="token operator">=</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss<span class="token operator">=</span>criterion<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算损失</span>    optm<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 前一步的损失清零</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 反向传播</span>    optm<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 优化</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token number">100</span> <span class="token operator">==</span><span class="token number">0</span> <span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 这里我们每100次输出相关的信息</span>        <span class="token comment" spellcheck="true"># 指定模型为计算模式</span>        net<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>        test_in<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>        test_l<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_lab<span class="token punctuation">)</span><span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span>        test_out<span class="token operator">=</span>net<span class="token punctuation">(</span>test_in<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 使用我们的测试函数计算准确率</span>        accu<span class="token operator">=</span>test<span class="token punctuation">(</span>test_out<span class="token punctuation">,</span>test_l<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch:{},Loss:{:.4f},Accuracy：{:.2f}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>accu<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Epoch:100,Loss:0.6313,Accuracy：0.76<br>Epoch:200,Loss:0.6065,Accuracy：0.79<br>Epoch:300,Loss:0.5909,Accuracy：0.80<br>Epoch:400,Loss:0.5801,Accuracy：0.81<br>Epoch:500,Loss:0.5720,Accuracy：0.82<br>Epoch:600,Loss:0.5657,Accuracy：0.81<br>Epoch:700,Loss:0.5606,Accuracy：0.81<br>Epoch:800,Loss:0.5563,Accuracy：0.81<br>Epoch:900,Loss:0.5527,Accuracy：0.81<br>Epoch:1000,Loss:0.5496,Accuracy：0.80</p><p>训练完成了，我们的准确度达到了80%</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch：循环神经网络</title>
      <link href="/2020/03/19/pytorch-xun-huan-shen-jing-wang-luo/"/>
      <url>/2020/03/19/pytorch-xun-huan-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=409650851&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="RNN简介"><a href="#RNN简介" class="headerlink" title="RNN简介"></a>RNN简介</h2><p>我们的大脑区别于机器的一个最大的特征就是我们有记忆，并且能够根据自己的记忆对未知的事务进行推导，我们的思想拥有持久性的。但是之前目前所介绍神经网络结构各个元素之间是相互独立的，输入与输出是独立的。</p><h2 id="RNN的起因"><a href="#RNN的起因" class="headerlink" title="RNN的起因"></a>RNN的起因</h2><p>现实世界中，很多元素都是互相连接的，比如室外的温度是随着气候的变化而周期性的变化的，我们的语言也需要通过上下文的关系来确认所表达的含义。<br>但是机器要做到这一步就相当得难了。因此，就有了现在的循环神经网络，本质是：拥有记忆的能力，并且会根据这些记忆的内容来进行推断。因此，他的输出就依赖于当前的输入和记忆。</p><h2 id="为什么需要RNN"><a href="#为什么需要RNN" class="headerlink" title="为什么需要RNN"></a>为什么需要RNN</h2><p>RNN背后的想法是利用顺序的信息。在传统的神经网络中，我们假设所有输入（和输出）彼此独立。如果你想预测句子中的下一个单词，你就需要知道它前面有哪些单词，甚至要看到后面的单词才能够给出正确的答案。RNN之所以成为循环，就是因为他们对序列的每个元素都会执行相同的任务，所有的输出都取决于先前的计算。从另一个角度讲RNN是有“记忆”的，可以捕获到目前为止计算的信息。理论上，RNN可以在任意长的序列中使用信息，但实际上他们仅限于回顾几个步骤。循环神经网络的提出便是基于记忆模型的想法。期望网络能够记住前面出现的特征，并根据特征推断后面的结果，而且整体的网络结构不断循环，因此得名循环神经网络</p><h2 id="RNN都能做什么"><a href="#RNN都能做什么" class="headerlink" title="RNN都能做什么"></a>RNN都能做什么</h2><p>RNN在许多NLP任务中取得了巨大成功。在这一点上，最常提到的RNN类型是LSTM,它在捕获长期依赖性方面要比RNN好得多。但不要担心，LSTM与我们将在本教程中开放的RNN基本相同，它们只是采用不同的方式来计算隐藏状态。我们将在后面更详细的介绍LSTM。以下是RNN在NLP中的一些示例：<strong>语言建模和生成文本</strong><br>我们通过语言的建模，可以通过给定的单词生成人类可以理解的以假乱真的文本</p><h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p>机器翻译类似于语言建模，我们的输入源语言中的一系列单词，通过模型的计算可以输出目标语言与之对应的内容。</p><h3 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h3><p>给定来自声波的声学信号的输入序列，我们可以预测一系列语音片段及其概率，并把语音转化成文字</p><h3 id="生成图像描述"><a href="#生成图像描述" class="headerlink" title="生成图像描述"></a>生成图像描述</h3><p>与卷积神经网络一起，RNN可以生成未标记图像的描述。</p><h2 id="RNN的网络结构及原理"><a href="#RNN的网络结构及原理" class="headerlink" title="RNN的网络结构及原理"></a>RNN的网络结构及原理</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>循环神经网络的基本结构特别简单，就是将网络的输出保存在一个记忆单元中，这个记忆单元和下一次的输入一起进入神经网络中。我们可以看到网络在输入的时候会联合记忆单元一起作为输入，网络不仅输出结果，还会将结果保存到记忆单元中，下图就是一个简单的循环神经网络在输入时的结构示意图。<img src="1.png" alt><br>RNN可以被看做是同一神经网络的多次赋值，每个神经网络模块会把消息传递给下一个，我们将这个图的结构展开<img src="2.png" alt><br>网络中具有循环结构，这也是循环神经网络名字的由来，同时根据循环神将网络的结构也可以看出它在处理序列类型的数据上具有天然的优势。因为网络本身就是一个序列结构，这也是所有循环神经网络最本质的结构。</p><p>循环神经网络具有特别好的记忆特性，能够将记忆内容应用到当前情景下，但是网络的记忆能力并没有想象的那么有效。记忆最大的问题在于它的遗忘性，我们总是更加清楚地记得最近发生的事情而以往很久以前发生的事情，循环神经网络同样有这样的问题。</p><p>pytorch中使用nn.RNN类来搭建基于序列的循环神经网络，它的构造函数有以下几个函数：</p><ul><li>nput_size: 输入数据X的特征值的数目。</li><li>hidden_size: 隐藏层的神经元数量，也就是隐藏层的特征数量。</li><li>num_layers: 循环神经网络的层数，默认值是1.</li><li>bias: 默认值为True，如果为false则表示神经元不适用bias偏移参数。</li><li>batch_first: 如果设置为True，则输入数据的维度中第一个维度就是batch值，默认为False。默认情况下第一个维度是序列的长度，第二个唯独才是–batch，第三个维度是特征数目。</li><li>droupout: 如果不为空，则表示最后跟一个dropout层抛弃部分数据，抛弃数据的比例由该参数指定。</li></ul><p>RNN中最主要的参数是input_size和hidden_size，这两个参数务必要搞清楚。其余的参数通常不用设置，采用默认值就可以了。</p><pre class="line-numbers language-python"><code class="language-python">rnn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>h_0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span>output<span class="token punctuation">,</span>hn <span class="token operator">=</span> rnn<span class="token punctuation">(</span>input<span class="token punctuation">,</span>h_0<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>hn<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([100, 32, 50]) torch.Size([2, 32, 50])</code></pre><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM是Long Shot Term Memory Networks的缩写，按字面翻译就是长的短时记忆网络。LSTM的网络结构是由1997年由Hochreiter和Schmidhuber提出的，随后这种网络结构变得非常流行。LSTM虽然只解决了短期依赖的问题，并且它通过刻意的设计来避免长期依赖的问题，这样的做法在实际应用中被证明是十分有效，很多人跟进相关的工作解决了很多实际的问题，所以现在LSTM仍然被广泛地使用。<img src="lstm.gif" alt><br>标准的循环神经网络内部只有一个简单的层结构，而LSTM内部有4个层结构：</p><p>第一层是个忘记层：决定状态中丢弃什么信息</p><p>第二层tanh层用来产生更新值的候选项，说明状态在某些维度上需要加强，在某些维度上需要减弱</p><p>第三层sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的状态不需要更新</p><p>最后一层决定输出什么，输出值跟状态有关。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。</p><p>pytorch中使用nn.LSTM类来搭建基于序列的循环神经网络，它的参数与RNN类似。</p><pre class="line-numbers language-python"><code class="language-python">lstm <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>h0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>c0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span> <span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>output<span class="token punctuation">,</span> hn <span class="token operator">=</span> lstm<span class="token punctuation">(</span>input<span class="token punctuation">,</span> <span class="token punctuation">(</span>h0<span class="token punctuation">,</span> c0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>hn<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>hn<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])</code></pre><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>GRU是gated recurrent units的缩写，由Cho在2014年提出。GRU和LSTM最大的不同在于GRU将遗忘门和输入们和成了一个“更新们”，同时为网络不再额外给出记忆状态，而是将输出结果作为记忆状态不断向后循环传递，网络的输入和输出都变得特别简单。<img src="gru.gif" alt></p><pre class="line-numbers language-python"><code class="language-python">rnn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>h_0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>output<span class="token punctuation">,</span> hb <span class="token operator">=</span> rnn<span class="token punctuation">(</span>input<span class="token punctuation">,</span> h0<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>h0<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([5, 3, 20]) torch.Size([2, 3, 20])</code></pre><h2 id="循环网络的向后传播（BPTT）"><a href="#循环网络的向后传播（BPTT）" class="headerlink" title="循环网络的向后传播（BPTT）"></a>循环网络的向后传播（BPTT）</h2><p>在向前传播的情况下，RNN的输入随着每一个时间步前进。在反向传播的情况下，我们“回到过去”改变权重，因此我们叫它通过时间的反向传播（BPTT).</p><p>我们通常把真个序列（单词）看做一个训练样本，所以总的误差是每个时间步（字符）中误差的和。权重在每一个时间步长是相同的（所以可以计算总误差后一起更新）。</p><p>1.使用预测输出和实际输出计算交叉熵误差<br>2.网络按照时间步完全展开<br>3.对于展开的网络，对于每一个时间不计算权重的梯度<br>4.因为对于所有时间步来说，权重都一样，所以对于所有的时间步，可以一起得到梯度（而不是像神经网络一样对不同的隐藏层得到不同的梯度）<br>5.随后对循环神经网络的权重进行训练</p><p>RNN展开的网络看起来像一个普通的神经网络。反向传播也类似于普通的神经网络，只不过我们一次得到所有时间步的梯度。如果有100个时间步，那么网络展开后将变得非常巨大，所以为了解决这个问题才会出现LSTM和GRU这样的结构。</p><p>循环神经网络目前在自然语言处理中应用最为火热，所以后面的内容将介绍一下循环神经网络在处理NLP的时候需要用到的一些其他的知识</p><h2 id="词嵌入（word-embedding）"><a href="#词嵌入（word-embedding）" class="headerlink" title="词嵌入（word embedding）"></a>词嵌入（word embedding）</h2><p>在我们人类交流过程中表征词汇是直接使用英文单词来进行表征的，但是对于计算机来说，是无法直接认识单词的。为了让计算机能够能更好地理解我们的语言，建立更好的语言模型，我们需要将词汇进行表征。</p><p>在图像分类问题会使用 one-hot 编码。比如LeNet中一共有10个数字0-9，如果这个数字是2的话类，它的编码就是 (0，0，1，0， 0，0 ，0，0，0，0)，对于分类问题这样表示十分的清楚，但是在自然语言处理中，因为单词的数目过多比如有 10000 个不同的词，那么使用 one-hot 这样的方式来定义，效率就特别低，每个单词都是 10000 维的向量。其中只有一位是 1 ， 其余都是 0，特别占用内存，而且也不能体现单词的词性，因为每一个单词都是 one-hot，虽然有些单词在语义上会更加接近.但是 one-hot 没办法体现这个特点，所以 必须使用另外一种方式定义每一个单词。</p><p>用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值这就是词嵌入。下图还是来自吴恩达老师的课程截图<img src="3.png" alt></p><p>词嵌入不仅对不同单词实现了特征化的表示，还能通过计算词与词之间的相似度，实际上是在多维空间中，寻找词向量之间各个维度的距离相似度，我们就可以实现类比推理，比如说夏天和热，冬天和冷，都是有关联关系的。</p><p>在 PyTorch 中我们用 nn.Embedding 层来做嵌入词袋模型，Embedding层第一个输入表示我们有多少个词，第二个输入表示每一个词使用多少维度的向量表示。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># an embedding model containing 10 tensors of size 3</span>embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># a batch of 2 samples of 4 indices each</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>output <span class="token operator">=</span> embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([2, 4, 3])</code></pre><h2 id="其它重要概念"><a href="#其它重要概念" class="headerlink" title="其它重要概念"></a>其它重要概念</h2><h3 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h3><p>在生成第一个词的分布后，可以使用贪心搜索会根据我们的条件语言模型挑选出最有可能输出的第一个词语，但是对于贪心搜索算法来说，我们的单词库中有成百到千万的词汇，去计算每一种单词的组合的可能性是不可行的。所以我们使用近似的搜索办法，使得条件概率最大化或者近似最大化的句子，而不是通过单词去实现。</p><p>Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。虽然Beam Search算法是不完全的，但是用于了解空间较大的系统中，可以减少空间占用和时间。</p><p>Beam search可以看做是做了约束优化的广度优先搜索，首先使用广度优先策略建立搜索树，树的每层，按照启发代价对节点进行排序，然后仅留下预先确定的个数（Beam width-集束宽度）的节点，仅这些节点在下一层次继续扩展，其他节点被剪切掉。</p><p>1.将初始节点插入到list中<br>2.将给节点出堆，如果该节点是目标节点，则算法结束；<br>3.否则扩展该节点，取集束宽度的节点入堆。然后到第二步继续循环。<br>4.算法结束的条件是找到最优解或者堆为空。</p><p>在使用上，集束宽度可以是预先约定的，也可以是变化的，具体可以根据实际场景调整设定。</p><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>对于使用编码和解码的RNN模型，我们能够实现较为准确度机器翻译结果。对于短句子来说，其性能是十分良好的，但是如果是很长的句子，翻译的结果就会变差。 我们人类进行人工翻译的时候，都是一部分一部分地进行翻译，引入的注意力机制，和人类的翻译过程非常相似，其也是一部分一部分地进行长句子的翻译。</p><p>具体的内容在这里就不详细介绍了</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch：卷积神经网络</title>
      <link href="/2020/03/04/pytorch-juan-ji-shen-jing-wang-luo-jian-jie/"/>
      <url>/2020/03/04/pytorch-juan-ji-shen-jing-wang-luo-jian-jie/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=31473269&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h1><p>卷积神经网络是由一个或多个卷积层和顶端的全连接层（也可以使用1×1的卷积层作为最终的输出）组成一种前馈神经网络。一般的认为，卷积神经网络是由Yann LeCun在1989年提出的LeNet中首先被使用，但是由于当时的计算能力不够，并没有得到广泛的应用，到了1998年Yann LeCun及其合作者构建了更加完备的卷积神经网络LeNet-5并在手写数字识别的问题中取得成功，LeNet-5基本上定义了现代卷积神经昂罗的基本结构，其构筑中交替出现的卷积层-池化层被认为有效提取了图像的平移不变特征，使得对于特征的提取前进了一大步，所以我们一般的认为，Yann LeCun是卷积神经网络的创始人。</p><p>2006年后，随着深度学习理论的完善，尤其是计算能力的提升和参数微调（fine-tuning)等技术的出现，卷积神经网络开始快速发展，在结构上不断加深，各类学习和优化理论得到引入，2012年的AlexNet、2014年的VGGNet、GoogLeNet和2015年的ResNet，使得卷积神经网络几乎成为了深度学习中图像处理方面的标配。</p><h2 id="为什么要使用卷积神经网络"><a href="#为什么要使用卷积神经网络" class="headerlink" title="为什么要使用卷积神经网络"></a>为什么要使用卷积神经网络</h2><p>对于计算机视觉来说，每一个图像是由一个个像素点构成，每个像素点有三个通道，分别代表RGB三种颜色（不计算透明度），我们以手写识别的数据集MNIST举例，每个图像的是一个长度为28，channel为1的单色图像，如果使用全连接的网络结构，即，网络中的神经与相邻层上的每个神经元均连接，那就意味着我们的网络有28×28=784个神经元（RGB3色的话还要3），hidden层如果使用了15个神经元，需要的参数个数（w和b）就有：28×28×15×10+15+10=117625个，这个数量级到现在为止也是一个很恐怖的事，一次反向传播计算量都是巨大的，这还展示一个单色的28像素大小的图片，如果我们使用更大的像素，计算量可想而知。</p><h2 id="结构组成"><a href="#结构组成" class="headerlink" title="结构组成"></a>结构组成</h2><p>上面说到传统的网络需要大量的参数，但是这些参数是否重复了呢，例如，我们识别一个人，只要看到他的眼睛，鼻子，嘴，还有脸基本上就知道这个人是谁了，只使用这些局部特征就能做判断了，并不需要所有的特征。另外一点就是我们上面说的可以有效提取了输入图像的平移不变特征，就好像我们看到了这个是眼睛，这个眼睛在左边还是在右边都是眼睛，这就是平移不变性。我们通过卷积的计算操作来提取图像局部的特征，每一层都会计算出一些局部特征，这些局部特征再汇总到下一层，这样一层层的传递下去，特征有小变大，最后在通过这些局部的特征对图片进行处理，这样大大提高了计算效率，也提高了准确度。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><h4 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h4><p>在介绍卷积层之前要先介绍一种卷积的技术，这里使用<a href="https://www.zhihu.com/question/39022858" target="_blank" rel="noopener">知乎</a>上的一张图片<br><img src="%E5%8D%B7%E7%A7%AF.gif" alt><br>我们会定义一个权重矩阵，也就是我们说的W(一般对于卷积来说，称做卷积的kernel也有有人称做过滤器filter），这个权重矩阵的大小一般为3×3或者5×5，但是在LeNet里面还用到了比较大的7×7，现在已经很少见了，因为根据经验的验证3和5是最佳的大小。我们以图上所示的方式，我们再输入矩阵上使用我们的权重矩阵进行滑动，没滑动一步，将所覆盖的值与矩阵对应的值相乘，并将结构求和并作为输出矩阵的一项，依此类推直到全部计算完成。</p><p>上图所示，我们输入是一个5×5的矩阵，通过使用一次的3<em>3的卷积核计算得到的计算结果是一个3</em>3的新矩阵。那么新矩阵的大小是如何计算的呢？</p><h4 id="卷积核大小f"><a href="#卷积核大小f" class="headerlink" title="卷积核大小f"></a>卷积核大小f</h4><p>刚才已经说到了一个重要的参数，就是核的大小，我们这里用f来表示</p><h4 id="边界填充（p-adding"><a href="#边界填充（p-adding" class="headerlink" title="边界填充（p)adding"></a>边界填充（p)adding</h4><p>我们看到上图，经过计算后矩阵的大小改变了，如果要使矩阵大小不改变呢，我们可以先对矩阵做一个填充，将矩阵的周围全部再包围一层，这个矩阵就变成了7<em>7，上下左右各加一，相当于5+1+1=7，这时，计算的结果还是5</em>5的矩阵，保证了大小不变，这里的p=1</p><h4 id="步长（s-tride"><a href="#步长（s-tride" class="headerlink" title="步长（s)tride"></a>步长（s)tride</h4><p>从动图上我们能够看到，每次滑动只是滑动了一个距离，如果每次滑动两个距离呢？那就需要使用步长这个参数。</p><h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><p>n为我们输入的矩阵的大小，(n-f+2p)/s+1向下取整<br>这个公式非常重要一定要记住</p><h4 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层"></a>卷积层</h4><p>在每一个卷积层中我们都会设置多个核，每个核代表着不同的特征，这些特征就是我们需要传递到下一层的输出，而我们训练的过程就是训练这些不同的核。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>由于卷积的操作也是线性的，所以也需要进行激活，一般情况下，都会使用relu。</p><h3 id="池化层（pooling"><a href="#池化层（pooling" class="headerlink" title="池化层（pooling)"></a>池化层（pooling)</h3><p>池化层是CNN的重要组成部分，通过减少卷积层之间的连接，降低运算复杂程度，池化层的操作很简单，就相当于是合并，我们输入一个过滤器的大小，与卷积的操作一样，也是一步步滑动，但是过滤器覆盖的区域进行合并，只保留一个值。合并的方式也有很多种。例如我们常用的两种取最大值maxpooling，取平均值avgpooling池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以p=0，可以简化为（n-f）/ s + 1</p><h4 id="dropout层"><a href="#dropout层" class="headerlink" title="dropout层"></a>dropout层</h4><p>dropout是2014年Hinton提出防止过拟合而采用的trick，增强了模型的泛化能力Dropout（随机失活）是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始网络中找到一个更瘦的网络，说的通俗一点，就是随机将一部分网络的传播掐断，听起来好像不靠谱，但是通过实际测试效果非常好。有兴趣的可以去看一下原文<a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>.</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>全连接层一般是作为最后的输出层使用，卷积的作用是提取图像的特征，最后的全连接层就是要通过这些特征来进行计算，输出我们所要的结果了，无论是分类，还是回归。<br>我们的特征都是使用矩阵表示的，所以在传入全连接层之前还需要对特征进行压扁，将他这些特征变成一位的向量，如果进行分类的话，就是用sofmax作为输出，如果要使回归的话就直接使用linear即可。<br>以上就是卷积神经网络的几个重要组成部分，下面我们介绍一些经典的网络模型</p><h2 id="经典模型"><a href="#经典模型" class="headerlink" title="经典模型"></a>经典模型</h2><p>LeNet-5<br>1998,Yann LeCun的LeNet5<a href="http://yann.lecun.com/exdb/lenet/index.html" target="_blank" rel="noopener">官网</a><br>卷积神经网络的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件</p><ul><li>用卷积层提取空间特征；</li><li>由空间平均得到子样本；</li><li>用tanh或sigmoid得到非线性</li><li>用multi-layer neural network(MLP)作为最终的分类器；</li><li>层层之间用稀疏的连接矩阵，以避免大的计算成本。<br><img src="lenet5.jpg" alt><br>输入：图像Size为32<em>32.这要比minist数据库中最大的字母（28</em>28）还大。这样做的目的是希望潜在的明显特征，如笔画断续、角点能够出现在最高层特征监测子感受野的中心。<br>输出：10个类别，分别是0-9数字的概率  </li></ul><p>1.C1层是一个卷积层，有6个卷积核（提取6中局部特征），核大小为5×5<br>2.S2层是pooling层，下采样（区域2×2）降低网络训练参数及模型的过拟合程度<br>3.C3层是第二个卷积层，使用16个卷积核，核大小：5×5提取特征<br>4.S4曾也是一个pooling层，区域：2×2<br>5.C5层是最后一个卷积层，卷积核大小：5×5卷积核种类：120<br>6.最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率<br>以下代码来自<a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" target="_blank" rel="noopener">官方教程</a></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">LeNet5</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>LeNet5<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 1 input image channel, 6 output channels, 5×5 square convolution</span>        <span class="token comment" spellcheck="true"># kernel</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># an affine operation: y = Wx + b</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#论文是conv，官方教程用了线性层</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Max pooling over a (2, 2) window</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#If the size is a square you can only specify a single number</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_flat_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">num_flat_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#all dimensions except the batch dimension</span>        num_fearures <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> size<span class="token punctuation">:</span>            num_fearuens <span class="token operator">*=</span> s        <span class="token keyword">return</span> num_featuresnet <span class="token operator">=</span> LeNet5<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>LeNet5(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>2012,<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Alex Krizhevsky</a>可以算作LeNet的一个更深和更广的版本，可以用来学习重复复杂的对象。</p><ul><li>用rectified linear units（ReLU)得到非线性；  </li><li>使用dropout技巧在训练期间有选择性地忽略单个神经元，来减缓模型的过拟合；  </li><li>重叠最大池，避免平均池的平均效果；  </li><li>使用GPU NVIDIA GTX 580可以减少训练时间，这比用CPU处理快了10倍，所以可以被用于更大的数据集和图像上。<br><img src="AlexNet.png" alt>  </li></ul><p>虽然AlexNet只有8层，但是它有60M以上的参数总量，Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理，这里就不作详细介绍了，AlexNet的每一阶段（含一次卷积主要计算的算作一层）可以分为8层：<br>1.con - relu - pooling - LRN ： 要注意的是input层是227×227，而不是paper里面的224，这里可以算一下，主要是227可以整除后面的conv1计算，224不整除。如果一定要用224可以通过自动补边实现，不过在input就补边感觉没有意义，补得也是0，这就是我们上面说的公式的重要性。<br>2.conv-relu-pool-LRN:group=2,这个属性强行把前面结果的feature map分开，卷积部分分成两部分做。<br>3.conv-relu<br>4.conv-relu<br>5.conv-relu-pool<br>6.fc-relu-droupout：droupout层，在alexnet中是说在训练的以1/2概率使得隐藏层的某些neuron的输出为0，这样就丢掉了一半节点的输出，BP的时候也不更新这些节点，防止过拟合。<br>7.fc-relu-dropout<br>8.fc-softmax<br>在PyTorch的vision包中是包含AlexNet的官方实现的，我们直接使用官方版本查看网络架构、</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>alexnet<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=4096, out_features=4096, bias=True)    (5): ReLU(inplace=True)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))</code></pre><p># </p><h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p>2015,牛津的<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">VGG</a></p><ul><li>每个卷积层中使用更小的3×3filters，并将它们合并组成卷积序列</li><li>多个3×3卷积序列可以模拟更大的接收场的效果</li><li>每次的图像像素缩小一倍，卷积核的数量增加一倍</li></ul><p>VGG有多个版本，也算是比较稳定和经典的模型。它的特点是连续conv多计算量巨大，我们这里以VGG16为例。<br><img src="vgg16.png" alt><br>VGG清一色用小卷积核，结合<a href="https://github.com/xxxnhb/pytorch-handbook/blob/master/chapter2/2.4-cnn.ipynb" target="_blank" rel="noopener">pytorch-handbook</a>的观点，这里整理出小卷积核比用大卷积核的优势：</p><p>input8 -&gt; 3层conv3x3后，output=2，等同于1层conv7x7的结果； input=8 -&gt; 2层conv3x3后，output=4，等同于1层conv5x5的结果</p><p>卷积层的参数减少。相比5×5、7×7和11×11的大卷积核，3×3明显地减少了参数量  </p><p>通过卷积和池化层后，图像的分辨率将为原来的一半，但是图像的特征增加了一倍，这是一个十分规整的操作：分辨率由输入的224-&gt;112-&gt;56-&gt;28-&gt;14-&gt;7,特征从原始的RGB3个通道-&gt;64-&gt;128-&gt;256-&gt;512</p><p>PyTorch官方实现版本</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>VGG(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace=True)    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU(inplace=True)    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (6): ReLU(inplace=True)    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (8): ReLU(inplace=True)    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (13): ReLU(inplace=True)    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (15): ReLU(inplace=True)    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (18): ReLU(inplace=True)    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (20): ReLU(inplace=True)    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (22): ReLU(inplace=True)    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (25): ReLU(inplace=True)    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (27): ReLU(inplace=True)    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (29): ReLU(inplace=True)    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))  (classifier): Sequential(    (0): Linear(in_features=25088, out_features=4096, bias=True)    (1): ReLU(inplace=True)    (2): Dropout(p=0.5, inplace=False)    (3): Linear(in_features=4096, out_features=4096, bias=True)    (4): ReLU(inplace=True)    (5): Dropout(p=0.5, inplace=False)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))</code></pre><h1 id="GoogLeNet-Inception"><a href="#GoogLeNet-Inception" class="headerlink" title="GoogLeNet(Inception)"></a>GoogLeNet(Inception)</h1><p>2014,<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Google Christian Szegedy</a></p><ul><li>使用1×1卷积块（NiN)来减少特征数量，这通常被称为“瓶颈”，可以减少神经网络的计算负担。  </li><li>每个池化层之前，增加feature maps，增加每一层的宽度来增多特征的组合性</li></ul><p>googlenet最大的特点就是包含若干个inception模块，所以有时候也称作inceptionnet。googlenet虽然层数上比vgg多很多，但是由于inception的设计，计算速度要快很多<br><img src="googlenet.png" alt><br>Inception架构的主要思想是找到如何让已有的稠密组件接近与覆盖卷积视觉网络中的最佳局部稀疏结构。现在需要找出最优的局部构造，并且重复几次。之前的一篇文献提出一个层与层的结构，在最后一层进行相关性统计，将高相关性的聚集到一起。这些聚类构成下一层的单元，且与上一层单元链接。假设前面层的每个单元对应于输入图像的某些区域，这些单元被分为滤波器组。在接近输入层的低层中，相关单元集中在某些局部区域，最终得到在单个区域中的大量聚类，在最后一层通过1×1的卷积覆盖。</p><p>上面的话听起来很生硬，其实解释起来很简单：每一模块我们都是用若干个不同的特征提取方式，例如3×3卷积，5×5卷积，1×1卷积，pooling等，都计算一下，最后再把这些结果通过Filter Concat来进行连接，找到这里面作用最大的。而网络里面包含了许多这样的模块，这样不用我们人为去判断哪个特征提取方式好，网络会自己解决（有点像AUTO ML),在PyTorch中实现了InceptionA-E,还有InceptionAUX模块。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># inception_v3 需要scipy，所以没有安装的话可以提前安装scipy</span><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>inception_v3<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Inception3(  (Conv2d_1a_3x3): BasicConv2d(    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_2a_3x3): BasicConv2d(    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_2b_3x3): BasicConv2d(    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_3b_1x1): BasicConv2d(    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Conv2d_4a_3x3): BasicConv2d(    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)  )  (Mixed_5b): InceptionA(    (branch1x1): BasicConv2d(      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_1): BasicConv2d(      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_2): BasicConv2d(      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_5c): InceptionA(    (branch1x1): BasicConv2d(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_1): BasicConv2d(      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_2): BasicConv2d(      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_5d): InceptionA(    (branch1x1): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_1): BasicConv2d(      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch5x5_2): BasicConv2d(      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6a): InceptionB(    (branch3x3): BasicConv2d(      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3): BasicConv2d(      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6b): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6c): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6d): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_6e): InceptionC(    (branch1x1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_2): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7_3): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_2): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_3): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_4): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7dbl_5): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (AuxLogits): InceptionAux(    (conv0): BasicConv2d(      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (conv1): BasicConv2d(      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (fc): Linear(in_features=768, out_features=1000, bias=True)  )  (Mixed_7a): InceptionD(    (branch3x3_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2): BasicConv2d(      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_1): BasicConv2d(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_2): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_3): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch7x7x3_4): BasicConv2d(      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_7b): InceptionE(    (branch1x1): BasicConv2d(      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_1): BasicConv2d(      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (Mixed_7c): InceptionE(    (branch1x1): BasicConv2d(      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_1): BasicConv2d(      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3_2b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_1): BasicConv2d(      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_2): BasicConv2d(      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3a): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch3x3dbl_3b): BasicConv2d(      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )    (branch_pool): BasicConv2d(      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)    )  )  (fc): Linear(in_features=2048, out_features=1000, bias=True))</code></pre><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>2015,<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Kaiming He,Xiangyu Zhang,Shaoqing Ren,Jian Sun.</a> </p><p>Kaiming He这个大神大家一定要记住,现在很多论文都有他参与（mask rcnn，focal loss），Jian Sun老师，旷视科技的首席科学家。googlenet已经很深了，ResNrt可以做到更深，通过残差计算，可以训练超过1000层的网络，俗称跳连接</p><h2 id="退化问题"><a href="#退化问题" class="headerlink" title="退化问题"></a>退化问题</h2><p>网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。这个就是网络退化的问题，退化问题说明了深度网络不能很简单地被很好地优化</p><h2 id="残差网络的解决方法"><a href="#残差网络的解决方法" class="headerlink" title="残差网络的解决方法"></a>残差网络的解决方法</h2><p>深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难。如果把网络设计为H(x) = F(x) + x。我们可以转换为学习一个残差函数F(x) = H(x) - x。 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。</p><p>以上又很不好理解，继续解释下，先看图：<br><img src="resnet.png" alt><br>我们在激活函数前将上一层（或几层）的输出与本层计算的输出相加，将求和的结果输入到激活函数中做为本层的输出，引入残差后的映射对输出的变化更敏感，其实就是看本层相对前几层是否有大的变化，相当于是一个差分放大器的作用。图中的曲线就是残差中的shoutcut，他将前一层的结果直接连接到了本层，也就是俗称的跳连接。<br>我们以经典的resnet18来看一下网络结构<br><img src="resnet18.jpg" alt></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmodel <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>ResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )    (1): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer2): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer3): Sequential(    (0): BasicBlock(      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer4): Sequential(    (0): BasicBlock(      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=512, out_features=1000, bias=True))</code></pre><h1 id="如何选择网络？"><a href="#如何选择网络？" class="headerlink" title="如何选择网络？"></a>如何选择网络？</h1><p><img src="cnn.png" alt><br>以上表格可以清楚的看到准确率和计算量之间的对比.小型图片分类任务，resnet18基本上已经可以了，如果真对准确度要求比较高，再选其他更好的网络架构。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch:数据加载和预处理</title>
      <link href="/2020/03/03/pytorch-shu-ju-de-jia-zai-he-yu-chu-li/"/>
      <url>/2020/03/03/pytorch-shu-ju-de-jia-zai-he-yu-chu-li/</url>
      
        <content type="html"><![CDATA[<h1 id="数据加载和预处理"><a href="#数据加载和预处理" class="headerlink" title="数据加载和预处理"></a>数据加载和预处理</h1><p>PyTorch通过torch.utils.data对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。并且torchvision已经预先实现了常用图像数据集，包括前面使用的CIDFAR-10,ImageNet、COCO、MNIST、LSUN等数据，可以通过torchvison.datasets方便的调用</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 首先要引入相关的包</span><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 打印一下版本</span>torch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>Dataset是一个抽象类，为了能够方便的读取，需要将使用的数据包装为Dataset类。自定义的Dataset需要继承它并且实现两个成员方法：<br>1.<strong>getitem</strong>()该方法定义用索引（0到len（self））获取一条数据或一个样本<br>2.<strong>len</strong>()该方法返回数据集的总长度<br>下面我们使用kaggel上的一个竞赛<a href="https://www.kaggle.com/c/bluebook-for-bulldozers/data" target="_blank" rel="noopener">bluebook for buildozers</a>自定义一个数据集，为了方便介绍，我们使用里面的数据字典来做说明</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 引用</span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#定义一个数据集</span><span class="token keyword">class</span> <span class="token class-name">BulldozerDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""数据集演示"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_file<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""实现初始化方法，在初始化的时候将数据读载入"""</span>        self<span class="token punctuation">.</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>csv_file<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        返回df长度        '''</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>df<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        根据idx返回一行数据        '''</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>df<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">.</span>SalePrice<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>至此，我们的数据集已经定义完成了，我们可以实例化一个对象访问它</p><pre class="line-numbers language-python"><code class="language-python">ds_demo <span class="token operator">=</span> BulldozerDataset<span class="token punctuation">(</span><span class="token string">'./bluebook-for-bulldozers/median_benchmark.csv'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 实现了__len__方法所以可以直接使用len获取数据总数</span>len<span class="token punctuation">(</span>ds_demo<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>11573</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#用索引可以直接访问对应的数据，对应__getitem__方法</span>ds_demo<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>24000.0</code></pre><p>自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据</p><h1 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h1><p>DataLoader为我们提供了对Dataset的读取操作，常用参数有：batch_size(每个batch的大小）、shuffle(是否进行shuffle操作)、num_workers(加载数据的时候使用几个子进程)。下面做一个简单的操作</p><pre class="line-numbers language-python"><code class="language-python">dl <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>ds_demo<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>DataLoader返回的是一个可迭代对象，我们可以使用迭代器分词获取数据</p><pre class="line-numbers language-python"><code class="language-python">idata <span class="token operator">=</span> iter<span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>next<span class="token punctuation">(</span>idata<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,        24000.])</code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>data<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#为了节约空间，这里只循环一遍</span>    <span class="token keyword">break</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0 tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,        24000.])</code></pre><p>我们以及可以使用dataset定义数据集，并使用Datalorder载入和遍历数据集，除了这些外，PyTorch还提供能torchvision的计算机视觉扩展包，里面封装了torchvision</p><h1 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h1><p>torchvision是PyTorch中专门用来处理图像的库，Pytorch官网的安装教程中最后的pip install torchvision就是安装这个包</p><h2 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h2><p>torchvision.datasets可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用</p><ul><li>MNIST    </li><li>COCO  </li><li>Captions  </li><li>Detection  </li><li>LSUN  </li><li>ImageFolder  </li><li>Imagenet-12  </li><li>CIFAR  </li><li>STL10  </li><li>SVHN  </li><li>Photo Tour我们可以直接使用，示例如下：</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> datasetstrainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#表示MNIST数据的加载的目录</span>                         train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#表示是否加载数据库的训练集，false的时候加载测试集</span>                         download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#表示是否自动下载MNIST数据集</span>                         transform<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#表示是否需要对数据进行预处理，none为不进行预处理</span>                         <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Files already downloaded and verified</code></pre><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者再进行迁移学习torchvision.models模块的子模块中包含以下模型结构。</p><ul><li>AlexNet</li><li>VGG</li><li>ResNet</li><li>SqueezeNet</li><li>DenseNet</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的</span><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> modelsresnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="torchvision-transfroms"><a href="#torchvision-transfroms" class="headerlink" title="torchvision.transfroms"></a>torchvision.transfroms</h3><p>transforms模块提供了一般的图像转换操作类，用作数据处理和数据增强</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms <span class="token keyword">as</span> transformstransform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#先四周填充0，再把图像随机裁剪成32*32</span>    transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 图像一半的改了旋转，一半的概率不旋转</span>    transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">45</span><span class="token punctuation">,</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#随机旋转</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.4914</span><span class="token punctuation">,</span> <span class="token number">0.4822</span><span class="token punctuation">,</span> <span class="token number">0.4465</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span><span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">#R、G、B每层的归一化用到的均值和方差</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010) 这几个数字的理解可以参考<a href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21" target="_blank" rel="noopener">官方</a>。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch:神经网络包和优化器optm</title>
      <link href="/2020/03/01/pytorch-shen-jing-wang-luo-bao-he-you-hua-qi-optm/"/>
      <url>/2020/03/01/pytorch-shen-jing-wang-luo-bao-he-you-hua-qi-optm/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络包和优化器optm"><a href="#神经网络包和优化器optm" class="headerlink" title="神经网络包和优化器optm"></a>神经网络包和优化器optm</h1><p>torch.nn是专门为神经网络设计的模块化接口。nn构建于Autograd之上，可以来定义和运行神经网络。这里我们主要介绍几个一些常用的类</p><p>约定：torch.nn我们为了方便使用，我们会把它设置为别名nn。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 首先要引入相关的包</span><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 引入torch.nn并指定别名</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment" spellcheck="true"># 打印一下版本</span>torch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><p>除了nn别名以外，我们还引用了nn.functional,这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数（ReLU,pool,DropOut等）,这些函数可以放在构造函数中，也可以不放，但是这里建议不放。<br>一般情况下我们会将nn.functional设置为F,方便调用。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h1><p>PyTorch中已经为我们准备好了现成的网络模型，只要继承nn.Model，并实现它的forward方法，PyTorch会根据autograd，自动实现backward函数，在forward函数中可以使用任何支持tensor的函数，还可以使用if、for循环、print、log等Python语法，写法和标准的Python语法一致。、</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># nn.Model子类的函数必须在构造函数中执行父类的构造函数</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 卷积层‘1’表示输入图片为单通道， ‘6’表示输出通道数， ‘3’表示卷积核为3*3</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 线性层，输入1350个特征，输出10个特征</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1350</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这里的1350根据后面forwar函数计算</span>    <span class="token comment" spellcheck="true">#正向传播</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 结果：【1， 1， 32， 32】</span>        <span class="token comment" spellcheck="true">#卷积 -> 激活 -> 池化</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 根据卷积的尺寸计算公式，计算结果是30</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#结果：【1，6，30，30】</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 我们使用池化层，计算结果是15</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># 结果：[1, 6, 15, 15]      </span>        <span class="token comment" spellcheck="true"># reshape, '-1'表示自适应</span>        <span class="token comment" spellcheck="true">#这里做的就是压扁操作，就是把后面的【1，6， 15， 15】压扁，变为【1，1350】</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#这里就是fc1层的输入1350</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))  (fc1): Linear(in_features=1350, out_features=10, bias=True))</code></pre><p>网络的可学习参数通过net.parameters()返回</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>parameters<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Parameter containing:tensor([[[[-0.2722,  0.2127,  0.1440],          [ 0.1893,  0.2289, -0.1173],          [ 0.1928,  0.0049, -0.0850]]],        [[[ 0.1248, -0.2400, -0.0258],          [ 0.0049,  0.0040, -0.2947],          [-0.2115,  0.1071,  0.2063]]],        [[[ 0.0656, -0.1588,  0.3105],          [-0.2901, -0.1213,  0.1705],          [-0.1190,  0.2064,  0.3216]]],        [[[ 0.1882,  0.3190,  0.3296],          [-0.3016, -0.2781,  0.1870],          [-0.2218, -0.1386, -0.0738]]],        [[[-0.0169,  0.2492,  0.0894],          [-0.2057, -0.2185,  0.0337],          [-0.2511,  0.1548,  0.0939]]],        [[[-0.0301, -0.2842,  0.3223],          [-0.0803,  0.0106, -0.0678],          [ 0.0725,  0.0646,  0.2251]]]], requires_grad=True)Parameter containing:tensor([-0.0749, -0.1380, -0.0128, -0.0752,  0.0611, -0.1311],       requires_grad=True)Parameter containing:tensor([[ 0.0081,  0.0077,  0.0227,  ..., -0.0003, -0.0174, -0.0037],        [-0.0006, -0.0046,  0.0208,  ...,  0.0218,  0.0025, -0.0128],        [-0.0166, -0.0182, -0.0263,  ...,  0.0062,  0.0203, -0.0041],        ...,        [ 0.0228,  0.0088,  0.0267,  ...,  0.0070, -0.0241,  0.0038],        [ 0.0180, -0.0091, -0.0142,  ...,  0.0152,  0.0247, -0.0197],        [ 0.0235, -0.0028, -0.0111,  ...,  0.0014,  0.0076,  0.0157]],       requires_grad=True)Parameter containing:tensor([-0.0089, -0.0139, -0.0196,  0.0216, -0.0125,  0.0169,  0.0131, -0.0029,        -0.0032,  0.0180], requires_grad=True)</code></pre><p>net.named_parameters可同时返回可学习的参数及名称</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span>parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">':'</span><span class="token punctuation">,</span>parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>conv1.weight : torch.Size([6, 1, 3, 3])conv1.bias : torch.Size([6])fc1.weight : torch.Size([10, 1350])fc1.bias : torch.Size([10])</code></pre><p>forward函数的输入输出都是Tensor</p><pre class="line-numbers language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 这里对应前面forward的输入是32</span>out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>out<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])torch.Size([1, 10])</code></pre><pre class="line-numbers language-python"><code class="language-python">input<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])</code></pre><p>在反向传播前，先要将所有参数的梯度清零</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。<br>也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的input，nn中定义为3维，但是我们人工创建时多增加了一个维度，变成了4维，最前面的1即为batch-size</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss来计算均方误差</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># loss是一个scalar，我们可以直接用item获取到他的python类型的数值</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>29.34039306640625</code></pre><h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>在反向传播计算完成所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法（SGD）的更新策略如下：<br>weight = weight-learning_rate*gradient<br>在torch.optim中实现大多数的优化方法，例如RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 这里调用的时候会打印出我们在forward函数中打印x的大小</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#新建一个优化器，SGD只需要调整的参数和学习率</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#先梯度清零（与net.zero_grad()效果一样）</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#更新参数</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([1, 1, 32, 32])torch.Size([1, 6, 30, 30])torch.Size([1, 6, 15, 15])torch.Size([1, 1350])</code></pre><p>这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch:MINIST数据集手写数字识别</title>
      <link href="/2020/03/01/pytorch-mnist-shou-xie-shu-zi-shi-bie/"/>
      <url>/2020/03/01/pytorch-mnist-shou-xie-shu-zi-shi-bie/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span>transformstorch<span class="token punctuation">.</span>__version__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&#39;1.3.0&#39;</code></pre><h1 id="MINIST数据集手写数字识别"><a href="#MINIST数据集手写数字识别" class="headerlink" title="MINIST数据集手写数字识别"></a>MINIST数据集手写数字识别</h1><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>MINIST包括6万张28×28的训练样本，1万张测试样本。</p><p>前面在介绍卷积神经网络的时候说到过LeNet-5，LeNet-5之所以强大就是因为在当时的环境下将MNIST数据的识别率提高到了99%，这里我们也自己从头搭建一个卷积神经网络，也达到99%的准确率</p><h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p>首先，我们定义一些超参数</p><pre class="line-numbers language-python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">512</span> <span class="token comment" spellcheck="true">#大概需要2G的显存</span>EPOCHS <span class="token operator">=</span> <span class="token number">20</span> <span class="token comment" spellcheck="true"># 总共训练批次</span>DEVICE <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>因为Pytorch里面包含了MNIST的数据集，所以我们这里直接使用即可。 如果第一次执行会生成data文件夹，并且需要一些时间下载，如果以前下载过就不会再次下载了</p><p>由于官方已经实现了dataset，所以这里可以直接使用DataLoader来对数据进行读取</p><pre class="line-numbers language-python"><code class="language-python">train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>        datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                      transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>                          transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                          transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.1307</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">0.3081</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                      <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">test_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>        datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'data'</span><span class="token punctuation">,</span>train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>            transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.1307</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                <span class="token punctuation">(</span><span class="token number">0.3081</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面我们定义一个网络，网络包含两个卷积层，conv1和conv2，然后紧接着两个线性层作为输出，最后输出10个维度，这10个维度我们作为0-9的标识来确定识别出的是那个数字</p><p>在这里建议大家将每一层的输入和输出维度都作为注释标注出来，这样后面阅读代码的会方便很多</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConvNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># batch*1*28*28(每次会送入batch个样本，输入通道1（黑白图像)，图像分辨率是28*28）</span>        <span class="token comment" spellcheck="true">#下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 输入通道数1，输出通道数10，核的大小5</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 输入通道数10，输出通道数20，核的大小3</span>        <span class="token comment" spellcheck="true"># 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token operator">*</span><span class="token number">10</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">500</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 输入通道数是2000，输出通道数是500</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 输入通道数是500，输出通道数是10，即10分类</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        in_size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 在本例中in_size=512，也就是BATCH_SIZE的值。输入的x可以看成是512*1*28*28的张量。</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*1*28*28 -> batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*10*24*24（激活函数ReLU不改变形状））</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>out<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*10*24*24 -> batch*10*12*12（2*2的池化层会减半）</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*10*12*12 -> batch*20*10*10（再卷积一次，核的大小是3）</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*20*10*10</span>        out <span class="token operator">=</span> out<span class="token punctuation">.</span>view<span class="token punctuation">(</span>in_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*20*10*10 -> batch*2000（out的第二维是-1，说明是自动推算，本例中第二维是20*10*10）</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*2000 -> batch*500</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*500</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># batch*500 -> batch*10</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算log(softmax(x))</span>        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们实例化一个网络，实例化后使用.to方法将网络移动到GPU</p><p>优化器我们也直接选择简单暴力的Adam</p><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> ConvNet<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>下面定义一下训练的函数，我们将训练的所有操作都封装到这个函数中</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        data<span class="token punctuation">,</span> target <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>batch_idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token number">30</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss:{:.6f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>            epoch<span class="token punctuation">,</span> batch_idx <span class="token operator">*</span> len<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">.</span> <span class="token operator">*</span> batch_idx <span class="token operator">/</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试的操作也一样封装成一个函数</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    test_loss <span class="token operator">=</span> <span class="token number">0</span>    correct <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> data<span class="token punctuation">,</span> target <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>            data<span class="token punctuation">,</span> target <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            test_loss <span class="token operator">+=</span> F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            pred <span class="token operator">=</span> output<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 找到概率最大的下标</span>            correct <span class="token operator">+=</span> pred<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>target<span class="token punctuation">.</span>view_as<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>    test_loss <span class="token operator">/=</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>        test_loss<span class="token punctuation">,</span> correct<span class="token punctuation">,</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token number">100</span><span class="token punctuation">.</span> <span class="token operator">*</span> correct <span class="token operator">/</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面开始训练，这里就体现出封装起来的好处了，只要写两行就可以了。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> EPOCHS <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> DEVICE<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span>    test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> DEVICE<span class="token punctuation">,</span> test_loader<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>Train Epoch: 1 [14848/60000 (25%)]    Loss:0.094219Train Epoch: 1 [30208/60000 (50%)]    Loss:0.073722Train Epoch: 1 [45568/60000 (75%)]    Loss:0.066013Test set: Average loss: 0.0617, Accuracy: 9808/10000 (98%)Train Epoch: 2 [14848/60000 (25%)]    Loss:0.086606Train Epoch: 2 [30208/60000 (50%)]    Loss:0.101871Train Epoch: 2 [45568/60000 (75%)]    Loss:0.048205Test set: Average loss: 0.0450, Accuracy: 9849/10000 (98%)Train Epoch: 3 [14848/60000 (25%)]    Loss:0.019774Train Epoch: 3 [30208/60000 (50%)]    Loss:0.033375Train Epoch: 3 [45568/60000 (75%)]    Loss:0.023632Test set: Average loss: 0.0358, Accuracy: 9883/10000 (99%)Train Epoch: 4 [14848/60000 (25%)]    Loss:0.022028Train Epoch: 4 [30208/60000 (50%)]    Loss:0.025542Train Epoch: 4 [45568/60000 (75%)]    Loss:0.037841Test set: Average loss: 0.0355, Accuracy: 9874/10000 (99%)Train Epoch: 5 [14848/60000 (25%)]    Loss:0.027415Train Epoch: 5 [30208/60000 (50%)]    Loss:0.011888Train Epoch: 5 [45568/60000 (75%)]    Loss:0.018164Test set: Average loss: 0.0391, Accuracy: 9869/10000 (99%)Train Epoch: 6 [14848/60000 (25%)]    Loss:0.026594Train Epoch: 6 [30208/60000 (50%)]    Loss:0.041575Train Epoch: 6 [45568/60000 (75%)]    Loss:0.013372Test set: Average loss: 0.0293, Accuracy: 9892/10000 (99%)Train Epoch: 7 [14848/60000 (25%)]    Loss:0.014221Train Epoch: 7 [30208/60000 (50%)]    Loss:0.014949Train Epoch: 7 [45568/60000 (75%)]    Loss:0.016649Test set: Average loss: 0.0367, Accuracy: 9867/10000 (99%)Train Epoch: 8 [14848/60000 (25%)]    Loss:0.008177Train Epoch: 8 [30208/60000 (50%)]    Loss:0.005532Train Epoch: 8 [45568/60000 (75%)]    Loss:0.025882Test set: Average loss: 0.0346, Accuracy: 9881/10000 (99%)Train Epoch: 9 [14848/60000 (25%)]    Loss:0.005148Train Epoch: 9 [30208/60000 (50%)]    Loss:0.003566Train Epoch: 9 [45568/60000 (75%)]    Loss:0.003365Test set: Average loss: 0.0287, Accuracy: 9905/10000 (99%)Train Epoch: 10 [14848/60000 (25%)]    Loss:0.003661Train Epoch: 10 [30208/60000 (50%)]    Loss:0.013061Train Epoch: 10 [45568/60000 (75%)]    Loss:0.007957Test set: Average loss: 0.0342, Accuracy: 9896/10000 (99%)Train Epoch: 11 [14848/60000 (25%)]    Loss:0.005907Train Epoch: 11 [30208/60000 (50%)]    Loss:0.005411Train Epoch: 11 [45568/60000 (75%)]    Loss:0.005319Test set: Average loss: 0.0356, Accuracy: 9903/10000 (99%)Train Epoch: 12 [14848/60000 (25%)]    Loss:0.008944Train Epoch: 12 [30208/60000 (50%)]    Loss:0.003442Train Epoch: 12 [45568/60000 (75%)]    Loss:0.007346Test set: Average loss: 0.0361, Accuracy: 9887/10000 (99%)Train Epoch: 13 [14848/60000 (25%)]    Loss:0.007296Train Epoch: 13 [30208/60000 (50%)]    Loss:0.003293Train Epoch: 13 [45568/60000 (75%)]    Loss:0.008293Test set: Average loss: 0.0289, Accuracy: 9911/10000 (99%)Train Epoch: 14 [14848/60000 (25%)]    Loss:0.004190Train Epoch: 14 [30208/60000 (50%)]    Loss:0.003393Train Epoch: 14 [45568/60000 (75%)]    Loss:0.000739Test set: Average loss: 0.0314, Accuracy: 9916/10000 (99%)Train Epoch: 15 [14848/60000 (25%)]    Loss:0.001715Train Epoch: 15 [30208/60000 (50%)]    Loss:0.003200Train Epoch: 15 [45568/60000 (75%)]    Loss:0.003006Test set: Average loss: 0.0340, Accuracy: 9901/10000 (99%)Train Epoch: 16 [14848/60000 (25%)]    Loss:0.001154Train Epoch: 16 [30208/60000 (50%)]    Loss:0.005416Train Epoch: 16 [45568/60000 (75%)]    Loss:0.000859Test set: Average loss: 0.0374, Accuracy: 9910/10000 (99%)Train Epoch: 17 [14848/60000 (25%)]    Loss:0.000601Train Epoch: 17 [30208/60000 (50%)]    Loss:0.007012Train Epoch: 17 [45568/60000 (75%)]    Loss:0.003185Test set: Average loss: 0.0367, Accuracy: 9897/10000 (99%)Train Epoch: 18 [14848/60000 (25%)]    Loss:0.000702Train Epoch: 18 [30208/60000 (50%)]    Loss:0.001315Train Epoch: 18 [45568/60000 (75%)]    Loss:0.001996Test set: Average loss: 0.0326, Accuracy: 9916/10000 (99%)Train Epoch: 19 [14848/60000 (25%)]    Loss:0.001052Train Epoch: 19 [30208/60000 (50%)]    Loss:0.002266Train Epoch: 19 [45568/60000 (75%)]    Loss:0.000495Test set: Average loss: 0.0461, Accuracy: 9882/10000 (99%)Train Epoch: 20 [14848/60000 (25%)]    Loss:0.008733Train Epoch: 20 [30208/60000 (50%)]    Loss:0.002210Train Epoch: 20 [45568/60000 (75%)]    Loss:0.000465Test set: Average loss: 0.0378, Accuracy: 9900/10000 (99%)</code></pre><p>我们看一下结果，准确率99%，没问题</p><p>如果你的模型连MNIST都搞不定，那么你的模型没有任何的价值</p><p>即使你的模型搞定了MNIST，你的模型也可能没有任何的价值</p><p>MNIST是一个很简单的数据集，由于它的局限性只能作为研究用途，对实际应用带来的价值非常有限。但是通过这个例子，我们可以完全了解一个实际项目的工作流程</p><p>我们找到数据集，对数据做预处理，定义我们的模型，调整超参数，测试训练，再通过训练结果对超参数进行调整或者对模型进行调整。</p><p>并且通过这个实战我们已经有了一个很好的模板，以后的项目都可以以这个模板为样例</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习：快速入门</title>
      <link href="/2020/02/29/pytorch-shen-du-xue-xi-kuai-su-ru-men/"/>
      <url>/2020/02/29/pytorch-shen-du-xue-xi-kuai-su-ru-men/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1426087898&auto=1&height=66"></iframe></div><h1 id="PyTorch-深度学习-60分钟快速入门-（官方）"><a href="#PyTorch-深度学习-60分钟快速入门-（官方）" class="headerlink" title="PyTorch 深度学习:60分钟快速入门 （官方）"></a>PyTorch 深度学习:60分钟快速入门 （官方）</h1><p>本章为官方网站的<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">Deep Learning with PyTorch: A 60 Minute Blitz</a>的中文翻译</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><a href="1_tensor_tutorial.ipynb">张量</a></li><li><a href="2_autograd_tutorial.ipynb">Autograd: 自动求导</a></li><li><a href="3_neural_networks_tutorial.ipynb">神经网络</a></li><li><a href="4_cifar10_tutorial.ipynb">训练一个分类器</a></li><li><a href="https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/5_data_parallel_tutorial.ipynb" target="_blank" rel="noopener">选读：数据并行处理</a></li></ol><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>本章中的所有图片均来自于PyTorch官网，版权归PyTorch所有.<br>为方便读者理解，相关内容以传送门形式在上方列出。</p><p><em>*</em> 本篇文章来自<a href="https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md" target="_blank" rel="noopener">Pytorch-handbook</a></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于CIFAR10训练一个分类器</title>
      <link href="/2020/02/28/4-cifar10-tutorial/"/>
      <url>/2020/02/28/4-cifar10-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27646687&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>Using matplotlib backend: agg</code></pre><h1 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h1><p>上一讲中已经看到如何去定义一个神经网络，计算损失值和更新网络的权重。你现在可能在想下一步。</p><h2 id="关于数据？"><a href="#关于数据？" class="headerlink" title="关于数据？"></a>关于数据？</h2><p>一般情况下处理图像、文本、音频数据时，可以使用标准的Python包来加载数据到一个numpy数组中。然后把这个数组转换成torcvh.*Tensor。</p><ul><li>图像可以使用Pilolow，OpenCV</li><li>音频可以使用scipy，librosa</li><li>文本可以使用原始Python和Cython来加载，或者使用NLTK或SpaCyc处理</li></ul><p>特别的，对于图像任务，我们创建了一个包torchvision,它包含了处理一些基本图像数据集的方法。这些数据集包括Imagenet、CIFAR10、MNIST等。除了数据加载以外，torchvision还包含了图像转换器，torchvision.datasets和torch.utils.data.DataLoader。<br>torchvision包不仅提供了巨大的便利，也避免了代码的重复。<br>在这个教程中，我们使用CIFAR10数据集，它有如下10个类别 ：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。CIFAR-10的图像都是 3x32x32大小的，即，3颜色通道，32x32像素。</p><h2 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h2><p>依次按照下列顺序进行：<br>1.使用torchvision加载和归一化CIFAR10训练集和测试机<br>2.定义一个卷积神经网络<br>3.定义损失函数<br>4.在训练集上训练网络<br>5.在测试集上测试网络  </p><h2 id="1-读取和归一化-CIFAR10"><a href="#1-读取和归一化-CIFAR10" class="headerlink" title="1.读取和归一化 CIFAR10"></a>1.读取和归一化 CIFAR10</h2><p>使用torchvision可以非常容易地加载CIFAR10。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>torchvision的输出是[0,1]的PILImage图像，我们把它转换为归一化范围为[-1，1]的张量</p><pre class="line-numbers language-python"><code class="language-python">transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>    <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>trainset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span>train <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                                       download <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> transform <span class="token operator">=</span> transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                                          shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>testset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                                       download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                                         shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span>           <span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>Tips：这里在CIFAR10官网已经提前下载好了数据集，和该文件放在了同一文件下，如电脑中没有CIFAR10数据集，则需将download参数置为True，root不指定参数<br>我们展示一些训练图像。<br><img src="cifar10.png" alt></li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true">#展示图像的函数</span><span class="token keyword">def</span> <span class="token function">imshow</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">:</span>    img <span class="token operator">=</span>img <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token comment" spellcheck="true"># unnormalize</span>    npimg <span class="token operator">=</span> img<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>npimg<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取随机数据</span>dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 展示图象</span>imshow<span class="token punctuation">(</span>torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示图像标签</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>  cat  ship  frog  bird</code></pre><h2 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2.定义一个卷积神经网络"></a>2.定义一个卷积神经网络</h2><p>从之前的神经网络一节复制神经网络代码，并修改为输入3通道图像。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3.定义损失函数和优化器"></a>3.定义损失函数和优化器</h2><p>我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimcriterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4.训练网络"></a>4.训练网络</h2><p>有趣的时刻开始了。我们只需在数据迭代器上循环，将数据输入给网络，并优化。</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>      <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 多批次循环</span>    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 获取输入</span>        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        <span class="token comment" spellcheck="true"># 梯度置0</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 正向传播，反向传播，优化</span>        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#打印状态信息</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2000</span> <span class="token operator">==</span> <span class="token number">1999</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 每2000批次打印一次</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span> <span class="token operator">%</span>                 <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> running_loss <span class="token operator">/</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Finished Training"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[1,  2000] loss: 2.058[1,  4000] loss: 1.772[1,  6000] loss: 1.639[1,  8000] loss: 1.547[1, 10000] loss: 1.485[1, 12000] loss: 1.442[2,  2000] loss: 1.371[2,  4000] loss: 1.357[2,  6000] loss: 1.310[2,  8000] loss: 1.315[2, 10000] loss: 1.281[2, 12000] loss: 1.260Finished Training</code></pre><h2 id="5-在测试集上测试网络"><a href="#5-在测试集上测试网络" class="headerlink" title="5.在测试集上测试网络"></a>5.在测试集上测试网络</h2><p>我们在整个训练集上进行了两次训练，但是我们需要检查网络是否从数据集中学习到有用的东西。通过预测神经网络输出的类标签与实际情况标签进行对比来进行检测。如果预测正确，我们把该样本添加到正确预测列表。第一步，显示测试集中图片并熟悉图片内容。</p><pre class="line-numbers language-python"><code class="language-python">dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>testloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示图片</span>imshow<span class="token punctuation">(</span>torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'GroundTruth: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>让我们看看神经网络认为以上图片是什么。</p><pre class="line-numbers language-python"><code class="language-python">output <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出是10个标签的能量。 一个类别的能量越大，神经网络越认为它是这个类别。所以让我们得到最高能量的标签。</p><pre class="line-numbers language-python"><code class="language-python">_<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Predicted: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>predicted<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span>                              <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Predicted:  horse  ship   dog   cat</code></pre><p>结果看来不错。</p><p>接下来让看看网络在整个测试集上的结果如何。</p><pre class="line-numbers language-python"><code class="language-python">correct <span class="token operator">=</span> <span class="token number">0</span>total <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy of the network on the 10000 test images: %d %%'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>    <span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Accuracy of the network on the 10000 test images: 55 %</code></pre><p>结果看起来不错，至少比随机选择要好，随机选择的正确率为10%。 似乎网络学习到了一些东西。</p><p>在识别哪一个类的时候好，哪一个不好呢？</p><pre class="line-numbers language-python"><code class="language-python">class_correct <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>class_total <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        c <span class="token operator">=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            label <span class="token operator">=</span> labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            class_correct<span class="token punctuation">[</span>label<span class="token punctuation">]</span> <span class="token operator">+=</span> c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            class_total<span class="token punctuation">[</span>label<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy of %5s : %2d %%'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>        classes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">100</span> <span class="token operator">*</span> class_correct<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/</span> class_total<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Accuracy of plane : 63 %Accuracy of   car : 74 %Accuracy of  bird : 17 %Accuracy of   cat : 26 %Accuracy of  deer : 50 %Accuracy of   dog : 49 %Accuracy of  frog : 70 %Accuracy of horse : 75 %Accuracy of  ship : 68 %Accuracy of truck : 55 %</code></pre><p>下一步？<br>我们如何在GPU上运行神经网络呢？</p><h2 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h2><p>把一个神经网络移动到GPU上训练就像把一个Tensor转换GPU上一样简单。并且这个操作会递归遍历有所模块，并将其参数和缓冲区转换为CUDA张量。</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 确认我们的电脑支持CUDA,然后显示CUDA信息</span><span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>cuda:0</code></pre><p>本节的其余部分假定device是CUDA设备。</p><p>然后这些方法将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量：</p><p>net.to(device)<br>记住：inputs, targets 和 images 也要转换。</p><p>inputs, labels = inputs.to(device), labels.to(device)<br>为什么我们没注意到GPU的速度提升很多？那是因为网络非常的小。</p><p>实践: 尝试增加你的网络的宽度（第一个nn.Conv2d的第2个参数，第二个nn.Conv2d的第一个参数，它们需要是相同的数字），看看你得到了什么样的加速。</p><h3 id="实现的目标："><a href="#实现的目标：" class="headerlink" title="实现的目标："></a>实现的目标：</h3><ul><li>深入了解了Pytorch的张量库和神经网络</li><li>训练了一个小网络来分类图片</li></ul><h2 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h2><p>如果你想使用所有的GPU得到更大的加速， 请查看数据并行处理。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络</title>
      <link href="/2020/02/28/3-neural-networks-tutorial/"/>
      <url>/2020/02/28/3-neural-networks-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1422274143&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>使用torch.nn包来构建神经网络<br>上一讲提到了autograd，nn包依赖autograd包来定义模型并求导。<br>一个nn.Model包含了各个层和一个forward(input)方法，该方法返回output。<br>例如：<br>它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。<br>神经网络的典型训练过程如下：<br>1.定义包含一些可学习的参数（或者叫权重）神经网络;<br>2.在数据集上迭代；<br>3.通过神经网络处理输入；<br>4.计算损失（输出结果和正确值的差值）；<br>5.将梯度反向传播回网络的参数；<br>6.更新网络的参数，主要使用如下简单的更新规则：<br>  weight = weight - learning_rate * gradient</p><h1 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h1><p>开始定义一个网络：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#I input image channel, 6 output channels, 5 × 5 squar e convolution</span>        <span class="token comment" spellcheck="true">#kernel</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># an affine operation: y = Wx + b</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#Max pooling over a (2, 2)window</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#If the size is a square you can only specify a single number</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_flat_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">num_flat_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#all dimensions except the batch dimension</span>        num_features <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> size<span class="token punctuation">:</span>            num_features <span class="token operator">*=</span> s        <span class="token keyword">return</span> num_featuresnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>在模型中必须要定义forward函数，backward函数（用来计算梯度）会被autograd自动创建。可以在forward函数中使用任何针对Tensor的操作</p><p>net.parameters()返回可被学习的参数（权重）列表和值</p><pre class="line-numbers language-python"><code class="language-python">params <span class="token operator">=</span> list<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>10torch.Size([6, 1, 5, 5])</code></pre><p>测试随机输入32*32。注：这个网络（LeNet)期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32.</p><pre class="line-numbers language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 0.0270, -0.1490, -0.0597, -0.0841, -0.0060, -0.1835,  0.0107,  0.0680,         -0.0480, -0.0272]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>将所有参数的梯度缓存清零，然后进行随机梯度的反向传播：</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>Note<br><code>torch.nn</code> 只支持小批量输入。整个 <code>torch.nn</code> 包都只支持小批量样本，而不支持单个样本。 例如，<code>nn.Conv2d</code> 接受一个4维的张量， <code>每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）</code>。 如果你有单个样本，只需使用 <code>input.unsqueeze(0)</code> 来添加其它的维数<br>在继续之前，我们回顾一下到目前为止用到的类。<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2></li><li>torch.Tensor:一个用过自动调用backward()实现支持自动梯度计算的多维数组，并且保存关于这个向量的梯度w.r.t.</li><li>nn.Module:神经网络模块。封装参数，移动到GPU上运行、导出、加载等。</li><li>nn.Parameter:一种变量，当把它赋给一个Module时，被自动地注册为一个参数</li><li>autograd.Function:实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个Tensor的操作都回创一个接到创建Tensor和编码其历史的函数的Function节点。<h2 id="重点如下："><a href="#重点如下：" class="headerlink" title="重点如下："></a>重点如下：</h2></li><li>定义一个网络</li><li>处理输入，调用backword<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1>一个损失函数接受一对（output，target）作为输入，计算一个值来估计网络的输出和目标值相差多少。<br>nn包中有很多不同的损失函数。nn.MSELoss是一个比较简单的损失函数，它计算输出和目标间的均方误差，例如：</li></ul><pre class="line-numbers language-python"><code class="language-python">output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> target <span class="token operator">=</span> target<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#使target和output的shape相同</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor(1.1544, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>现在，如果在反向过程中跟随loss，使用它的.grad_fn属性，将看到如下所示的计算图。<br>::<br>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss<br>所以，当我们调用loss.backward()时，整张计算图都会根据loss进行微分，而且图中所有设置为requires_grad=True的张量将会拥有一个随着梯度累积的.grad张量。</p><p>为了说明，让我们后退几步：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>&lt;MseLossBackward object at 0x7f39c7490710&gt;&lt;AddmmBackward object at 0x7f39c7490910&gt;&lt;AccumulateGrad object at 0x7f3a13de12d0&gt;</code></pre><h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>调用loss.backward()获得反向传播的误差。</p><p>但是在调用前需要清楚已存在的梯度，否则梯度将被累加到已存在的梯度。</p><p>现在，我们将调用loss.backward(),并查看conv1层的偏差（bias）项在反向传播前后的梯度</p><pre class="line-numbers language-python"><code class="language-python">net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"conv1.bias.grad before backward"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"conv1.bias.grad after backward"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>conv1.bias.grad before backwardtensor([0., 0., 0., 0., 0., 0.])conv1.bias.grad after backwardtensor([0.0200, 0.0075, 0.0097, 0.0054, 0.0115, 0.0240])</code></pre><p>如何使用损失函数</p><h3 id="稍后阅读："><a href="#稍后阅读：" class="headerlink" title="稍后阅读："></a>稍后阅读：</h3><p>nn包，包含了各种用来构成深度神经网络构建的模块和损失函数，完整的文档请查看<a href="https://pytorch.org/docs/nn" title="Torch.nn" target="_blank" rel="noopener">这里</a></p><h1 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h1><p>在实践中最简单的权重更新规则是随机梯度下降（SGD):<br>weight = weight - learning_rate * gradient  </p><p>我们可以使用简单的Python代码实现这个规则：</p><pre><code>learning_rate = 0.01for f in net.parameters():    f.data.sub_(f.grad.data * learning_rate)</code></pre><p>但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包torch.optim实现了所有这些规则。使用它们非常简单：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token comment" spellcheck="true">#create your optimizer</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#in your training loop:</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#zero the gradient buffers</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true">#Does the update</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>观察如何使用“optimizer.zero_grad(“手动将梯度缓冲区设置为零。<br>这是因为梯度是按Backprop部分中的说明累积的。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自动求导(Autograd)</title>
      <link href="/2020/02/28/2-autograd-tutorial/"/>
      <url>/2020/02/28/2-autograd-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1398305777&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="Autograd-自动求导机制"><a href="#Autograd-自动求导机制" class="headerlink" title="Autograd:自动求导机制"></a>Autograd:自动求导机制</h1><p>PyTorch中所有神经网络的核心是autograd包。我们先简单介绍一下这个包，然后训练第一个简单的神经网络。<br>autograd包为张量上的所有操作提供了自动求导。它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。<br>示例  </p><h1 id="张量（Tensor"><a href="#张量（Tensor" class="headerlink" title="张量（Tensor)"></a>张量（Tensor)</h1><p>torch.Tensor是这个包的核心类。如果设置。requires_grad为true，那么将会追踪所有对于该张量的操作。当完成计算后通过调用.backward(),自动计算所有的梯度，这个张量的所有梯度将会自动积累到.grad属性。</p><p>要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。  </p><p>为了防止跟踪历史记录（和使用内存）可以将代码块包装在withtouch.no_grad():中。在评估模型时特别有用，因为模型可能具有requires_grad = True的可训练参数，但是我们不需要梯度计算。</p><p>在自动梯度计算中还有另一个重要的类Function.Tensor和Function互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个.grad_fn属性，这个属性引用了一个创建了Tensor的Function(除非这个张量是用户手动创建的，即，这个张量的grad_fn是None).</p><p>如果需要计算导数，你可以在Tensor上调用.bvackward()。如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward（）指定任何参数，<br>但是如果它有更多的元素，你需要指定一个gradient参数来匹配张量的形状。<br><a href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated" target="_blank" rel="noopener">官方文档</a></p><p>具体的后面会有详细说明</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>创建一个张量并设置requires_grad=True用来追踪它的计算历史</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[1., 1.],        [1., 1.]], requires_grad=True)</code></pre><p>对张量进行操作：</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p>结果y已经被计算出来了，所以，grad_fn已经被自动生成了</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&lt;AddBackward0 object at 0x7fbb2e4392d0&gt;</code></pre><p>对y进行一个操作</p><pre class="line-numbers language-python"><code class="language-python">z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> out<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[27., 27.],        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p>.requires_grad_(…)可以改变现有张量的requires_grad属性。如果没有指定的话，默认输入的flag是False。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>a <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>a<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>b <span class="token operator">=</span> <span class="token punctuation">(</span>a <span class="token operator">*</span> a<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>FalseTrue&lt;SumBackward0 object at 0x7fbb2e439ed0&gt;</code></pre><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>反向传播 因为out是一个纯量（scalar），out.backward()等于out.backward(torch.tensor(1)).</p><pre class="line-numbers language-python"><code class="language-python">out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>print gradients d(out)/dx</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[4.5000, 4.5000],        [4.5000, 4.5000]])</code></pre><p>得到矩阵4.5.将out叫做Tensor“o”<br><img src="Autograd.jpg" alt><br>现在让我们来看一下vector-Jacobian product的例子</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token keyword">while</span> y<span class="token punctuation">.</span>data<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>    y <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([-1578.2891,  -373.9413,   691.9827], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>在这个情形中，y不再是个标量。torch.autograd无法直接计算出完整的雅可比行列式，但是如果我们只想要vector-Jacobian product,只需将向量作为参数传入backward：</p><pre class="line-numbers language-python"><code class="language-python">gradients <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.0001</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>gradients<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</code></pre><p>如果.requires_grad=True但是你又不希望进行autograd的计算，那么可以将变量包裹在with torch.no_grad()中：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>TrueTrueFalse</code></pre><ul><li>稍后阅读：<br>autograd 和 Function的<a href="https://pytorch.org/docs/autograd" title="PyTorch" target="_blank" rel="noopener">官方文档</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>张量(Tensor)简介</title>
      <link href="/2020/02/28/1-tensor-tutorial/"/>
      <url>/2020/02/28/1-tensor-tutorial/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=475530855&auto=1&height=66"></iframe></div><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="PyTorch是什么？"><a href="#PyTorch是什么？" class="headerlink" title="PyTorch是什么？"></a>PyTorch是什么？</h1><p>基于Python的科学计算包，服务于以下两种场景</p><ul><li>作为NumPy的替代品，可以使用GPU的强大计算能力</li><li>提供最大的灵活性和高速的深度学习研究平台</li></ul><h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><p>Tensors(张量)</p><p>Tensors与Numpy中的ndarrays类似，但是在PyTorch中Tensors可以使用GPU进行计算。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> print_function<span class="token keyword">import</span> torch<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>创建一个5×3矩阵，但是未初始化</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[-7.2630e-28,  4.5729e-41, -7.2630e-28],        [ 4.5729e-41,         nan,  3.0970e-41],        [ 1.7753e+28,  4.4339e+27,  5.6719e-11],        [ 7.3471e+28,  2.6383e+23,  2.7376e+20],        [ 1.8040e+28,  1.8750e-19,  7.3909e+22]])</code></pre><p>创建一个随机初始化的矩阵</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0.6957, 0.8565, 0.7844],        [0.3198, 0.9460, 0.4244],        [0.7566, 0.7370, 0.6519],        [0.7324, 0.3396, 0.7447],        [0.1074, 0.1667, 0.0326]])</code></pre><p>创建一个0填充的矩阵，数据类型为long</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]])</code></pre><p>创建tensor并使用现有数据初始化</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([5.5000, 3.0000])</code></pre><p>使用现有的张量创建张量。这些方法将重用输入张量的属性，例如，dtype,除非设置新的值进行覆盖</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> x<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#new_*方法来创建对象</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#覆盖dtype!</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>                                    <span class="token comment" spellcheck="true">#d对象的size相同，值和类型发生变化  </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]], dtype=torch.float64)tensor([[ 0.2724, -2.0907, -0.6096],        [-0.3521,  1.9110, -0.0053],        [ 1.6634,  1.4252, -0.0778],        [ 1.1623, -0.3941, -0.8029],        [ 0.8197,  0.0218, -1.7554]])</code></pre><p>获取size</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>torch.Size([5, 3])</code></pre><p>“torch.Size”返回值是tuple类型，支持tuple类型的所有操作</p><p>加法1：</p><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token operator">+</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>加法2</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>提供输出tensor作为参数(结果被填充到result作为参数）</p><pre class="line-numbers language-python"><code class="language-python">result <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>out<span class="token operator">=</span>result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]])tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>替换（任何以“_”结尾的操作都会用结果替换原变量）</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#add x to y</span>y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([[ 3.0160e-01, -1.1085e+00,  3.4606e-01],        [ 4.3867e-02,  2.1488e+00,  4.5966e-01],        [ 2.5710e+00,  1.6776e+00,  8.5985e-01],        [ 1.8511e+00, -2.2450e-01, -1.7348e-03],        [ 1.2764e+00,  2.9380e-01, -1.3293e+00]])</code></pre><p>你可以使用与NumPy索引方式相同的操作来进行对张量的操作</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>tensor([-2.0907,  1.9110,  1.4252, -0.3941,  0.0218])</code></pre><p>torch.view():可以改变张量的维度和大小</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span>z <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#size -1 从其他维度推断</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>如果你只有一个元素的张量，使用.item()来得到Python数据类型的数值</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([1.0249])1.0248878002166748</code></pre><p>如果你想要了解更多的关于Tensor的操作，可以访问<a href="https://pytorch.org/docs/torch" title="Tensors" target="_blank" rel="noopener">PyTorch官方文档</a></p><h1 id="NumPy转换"><a href="#NumPy转换" class="headerlink" title="NumPy转换"></a>NumPy转换</h1><p>将一个Torch Tensor转换为NumPy数组是一件轻松的事，反之亦然。<br>Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。</p><p>将一个Torch Tensor转换为NumPy数组</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>tensor([1., 1., 1., 1., 1.])</code></pre><pre class="line-numbers language-python"><code class="language-python">b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>[1. 1. 1. 1. 1.]</code></pre><p>观察numpy数组的值是如何改变的。</p><pre class="line-numbers language-python"><code class="language-python">a<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>tensor([2., 2., 2., 2., 2.])[2. 2. 2. 2. 2.]</code></pre><p>NumPy Array转化成Torch Tensor  </p><p>使用from_numpy自动转化</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span>np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> out<span class="token operator">=</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[2. 2. 2. 2. 2.]tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><p>所有的Tensor类型默认都是基于CPU，CharTensor类型不支持到NumPy的转换。</p><h1 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h1><p>使用.to方法可以将Tensor移动到任何设备中</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#is_available函数判断是否有cuda可以使用</span><span class="token comment" spellcheck="true">#``torch.device``将张量移动到指定的设备中</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true">#a CUDA 设备对象</span>    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#直接从GPU创建张量</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                         <span class="token comment" spellcheck="true">#直接使用``.to（"cuda"）``将张量移动到cuda中</span>    z <span class="token operator">=</span> x <span class="token operator">+</span> y    <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true">#``.to``也会对变量的类型做更改</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>tensor([2.0249], device=&#39;cuda:0&#39;)tensor([2.0249], dtype=torch.float64)</code></pre><p>感谢您的浏览</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch快速入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch简介及环境搭建</title>
      <link href="/2020/02/27/pytorch-jian-jie-ji-huan-jing-da-jian/"/>
      <url>/2020/02/27/pytorch-jian-jie-ji-huan-jing-da-jian/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1417442423&auto=1&height=66"></iframe></div><p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。</p><h1 id="1-Pytorch简介"><a href="#1-Pytorch简介" class="headerlink" title="1.Pytorch简介"></a>1.Pytorch简介</h1><hr><p>很多人都会拿PyTorch和Google的Tensorflow进行比较，它们是最火的两个深度学习框架了。<br>谈到PyTorch，我们应该先说<a href="http://torch.ch" target="_blank" rel="noopener">Torch</a>。Torch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。<br>PyTorch和Torch使用包含所有相同性能的C库：TH, THC, THNN, THCUNN，并且它们将继续共享这些库。<br>这样的回答就很明确了，其实PyTorch和Torch都使用的是相同的底层，只是使用了不同的上层包装语言。</p><p>PyTorch是一个Python包，提供两个高级功能：</p><ul><li>具有强大的GPU加速的张量计算（如NumPy）</li><li>包含自动求导系统的的深度神经网络</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>PyTorch是相当简洁优雅且高效快速的框架</li><li>设计追求最少的封装，尽量避免重复造轮子</li><li>算是所有的框架中面向对象设计的最优雅的一个，设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法</li><li>大佬支持,与google的Tensorflow类似，FAIR的支持足以确保PyTorch获得持续的开发更新</li><li>不错的的文档（相比FB的其他项目，PyTorch的文档简直算是完善了，参考Thrift），PyTorch作者亲自维护的论坛 供用户交流和求教问题</li><li>入门简单</li></ul><h1 id="2-Pytorch环境搭建"><a href="#2-Pytorch环境搭建" class="headerlink" title="2.Pytorch环境搭建"></a>2.Pytorch环境搭建</h1><p>PyTorch的安装十分简单，根据<a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch官网</a>，对系统选择和安装方式等灵活选择即可。<br>这里以<a href="https://www.anaconda.com/" target="_blank" rel="noopener">anaconda</a>为例，简单的说一下步骤和要点。<br>国内安装anaconda建议使用<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="noopener">清华</a>镜像。</p><h2 id="2-1安装Pytorch"><a href="#2-1安装Pytorch" class="headerlink" title="2.1安装Pytorch"></a>2.1安装Pytorch</h2><p>pytorch的安装经过了几次变化，请大家以官网的安装命令为准。另外需要说明的就是在1.2版本以后，pytorch只支持cuda 9.2以上了，所以需要对cuda进行升级，目前测试大部分显卡都可以用，包括笔记本的MX250也是可以顺利升级到cuda 10.1。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#默认 使用 cuda10.1</span>pip <span class="token function">install</span> torch<span class="token operator">==</span><span class="token operator">=</span>1.4.0 torchvision<span class="token operator">==</span><span class="token operator">=</span>0.5.0 -f https://download.pytorch.org/whl/torch_stable.html<span class="token comment" spellcheck="true">#cuda 9.2</span>pip <span class="token function">install</span> torch<span class="token operator">==</span>1.4.0+cu92 torchvision<span class="token operator">==</span>0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html<span class="token comment" spellcheck="true">#cpu版本</span>pip <span class="token function">install</span> torch<span class="token operator">==</span>1.4.0+cpu torchvision<span class="token operator">==</span>0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>验证输入python 进入</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__<span class="token comment" spellcheck="true"># 得到结果'1.4.0'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-2配置Jupyter-Notebook"><a href="#2-2配置Jupyter-Notebook" class="headerlink" title="2.2配置Jupyter Notebook"></a>2.2配置Jupyter Notebook</h2><p>新建的环境是没有安装安装ipykernel的所以无法注册到Jupyter Notebook中，所以先要准备下环境</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#安装ipykernel</span>conda <span class="token function">install</span> ipykernel<span class="token comment" spellcheck="true">#写入环境</span>python -m ipykernel <span class="token function">install</span>  --name pytorch --display-name <span class="token string">"Pytorch for Deeplearning"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>下一步就是定制 Jupyter Notebook</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#切换回基础环境</span>activate base<span class="token comment" spellcheck="true">#创建jupyter notebook配置文件</span>jupyter notebook --generate-config<span class="token comment" spellcheck="true">## 这里会显示创建jupyter_notebook_config.py的具体位置</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>打开文件，修改</p><pre><code>c.NotebookApp.notebook_dir = &#39;&#39; 默认目录位置c.NotebookApp.iopub_data_rate_limit = 100000000 这个改大一些否则有可能报错</code></pre><h2 id="2-3-测试"><a href="#2-3-测试" class="headerlink" title="2.3 测试"></a>2.3 测试</h2><p>至此 Pytorch 的开发环境安装完成，可以在开始菜单中打开Jupyter Notebook 在New 菜单中创建文件时选择<code>Pytorch for Deeplearning</code> 创建PyTorch的相关开发环境了</p><h2 id="2-4-问题解决"><a href="#2-4-问题解决" class="headerlink" title="2.4 问题解决"></a>2.4 问题解决</h2><h3 id="问题1：启动python提示编码错误"><a href="#问题1：启动python提示编码错误" class="headerlink" title="问题1：启动python提示编码错误"></a>问题1：启动python提示编码错误</h3><p>删除 .python_history <a href="http://tantai.org/posts/install-keras-pytorch-jupyter-notebook-Anaconda-window-10-cpu/" target="_blank" rel="noopener">来源</a></p><h3 id="问题2-默认目录设置不起效"><a href="#问题2-默认目录设置不起效" class="headerlink" title="问题2 默认目录设置不起效"></a>问题2 默认目录设置不起效</h3><p>打开快捷方式，看看快捷方式是否跟这个截图一样，如果是则删除 <code>%USERPROFILE%</code> 改参数会覆盖掉notebook_dir设置，导致配置不起效<br><img src="pic1.png" alt="Alt text"></p><p>本文参考PyTorch中文手册<br>如果你还发现其他问题，请直接留言</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
